{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_time(dataset):\n",
    "  load_times = []\n",
    "  end = time.time()\n",
    "  for batch in dataset:\n",
    "      load_times.append(time.time() - end)\n",
    "      end=time.time()\n",
    "\n",
    "  load_times = np.array(load_times)\n",
    "  print(f\"Avg load time: {np.mean(load_times)}, std: {np.std(load_times)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataset = datasets.load_from_disk(\"dataset/hf_test/scripted_trajectories_50_2024-07-14_14-25-22.hf\").with_format(\"torch\")\n",
    "mydataloader = DataLoader(mydataset, batch_size=256, shuffle=True, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg load time: 0.0015681628704071046, std: 0.000694624684764919\n"
     ]
    }
   ],
   "source": [
    "test_load_time(mydataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg load time: 0.11769705707744016, std: 0.46098371012284395\n"
     ]
    }
   ],
   "source": [
    "test_load_time(mydataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_gripper(gripper):\n",
    "  \"\"\"\n",
    "  Convert from (-1, 1) to one hot encoded\n",
    "  One hot needs them as 1d\n",
    "  \"\"\"\n",
    "  return torch.nn.functional.one_hot(gripper.flatten() + 1, num_classes=3)\n",
    "\n",
    "def decode_gripper(gripper):\n",
    "  \"\"\"\n",
    "  Convert from one hot encoded to column vector in range (-1, 1)\n",
    "  \"\"\"\n",
    "  return (gripper.argmax(dim=1) - 1).unsqueeze(1).to(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset.map()\n",
    "V slow\n",
    "\n",
    "Calling .map() is cached for most of these, typically takes around 1m20s.\n",
    "\n",
    "The image normalisation seens to cause the vast majority of the slowdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bounds_centre = torch.tensor([0]*6)\n",
    "bounds_range = torch.tensor([12]*6)\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    \"\"\"\n",
    "    Take a batch of data and put it in a suitable tensor format for the model\n",
    "    \"\"\"\n",
    "    def normalize_qpos(qpos):\n",
    "        return (qpos - bounds_centre) / bounds_range + 0.5\n",
    "    \n",
    "    observation_qpos_normalised = torch.atleast_2d(normalize_qpos(batch[\"observation.state.qpos\"]).to(torch.float32))\n",
    "    observation_gripper = torch.atleast_2d(embed_gripper(batch[\"observation.state.gripper\"]).to(torch.float32))\n",
    "    \n",
    "    observation_state = torch.hstack((observation_qpos_normalised, observation_gripper))\n",
    "\n",
    "    action_qpos_normalised = torch.atleast_2d(normalize_qpos(batch[\"action.qpos\"]).to(torch.float32))\n",
    "    action_gripper = torch.atleast_2d(embed_gripper(batch[\"action.gripper\"]).to(torch.float32))\n",
    "    action_state = torch.hstack((action_qpos_normalised, action_gripper))\n",
    "    \n",
    "    image = batch[\"observation.pixels\"]/ 255\n",
    "\n",
    "    batch = {\"preprocessed.observation.state\": observation_state, \"preprocessed.observation.image\": image,\n",
    "             \"preprocessed.action.state\": action_state}\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Initial: 1m44 to iterate throfu\n",
    "# Remove .to(torch.float32): 2m16s to run map, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.profiler.profile(on_trace_ready=torch.profiler.tensorboard_trace_handler(\"runs/dataload_profile\")) as p:\n",
    "  mydataset = mydataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                aten::reshape         0.00%      11.000us         0.00%      48.000us      24.000us             2  \n",
      "                   aten::view         0.00%      83.000us         0.00%      83.000us       4.882us            17  \n",
      "                 aten::unbind         0.16%       4.269ms         0.16%       4.433ms     554.125us             8  \n",
      "                 aten::select         0.00%     134.000us         0.01%     159.000us       0.026us          6012  \n",
      "             aten::as_strided         0.00%      50.000us         0.00%      50.000us       0.008us          6055  \n",
      "                   aten::item         0.00%      40.000us         0.00%      44.000us       0.611us            72  \n",
      "    aten::_local_scalar_dense         0.00%      10.000us         0.00%      10.000us       0.139us            72  \n",
      "           aten::resolve_conj         0.00%       1.000us         0.00%       1.000us       0.011us            94  \n",
      "            aten::resolve_neg         0.00%       0.000us         0.00%       0.000us       0.000us            94  \n",
      "                 aten::detach         0.01%     146.000us         0.01%     346.000us       1.880us           184  \n",
      "                       detach         0.01%     217.000us         0.01%     217.000us       1.179us           184  \n",
      "                     aten::to         2.68%      73.010ms        37.06%        1.010s      65.187us         15499  \n",
      "             aten::lift_fresh         0.00%      20.000us         0.00%      20.000us       0.001us         15135  \n",
      "               aten::_to_copy         1.23%      33.407ms        36.88%        1.005s      65.779us         15285  \n",
      "          aten::empty_strided         0.02%     467.000us         0.02%     467.000us       0.031us         15285  \n",
      "                  aten::copy_        36.29%     989.373ms        36.29%     989.373ms      64.728us         15285  \n",
      "                aten::detach_         0.01%     321.000us         0.01%     363.000us       0.024us         15135  \n",
      "                      detach_         0.00%      44.000us         0.00%      44.000us       0.003us         15135  \n",
      "                  aten::stack         0.04%       1.016ms        29.16%     794.908ms      52.994ms            15  \n",
      "                    aten::cat        29.13%     794.155ms        29.13%     794.264ms      17.650ms            45  \n",
      "                 aten::narrow         0.00%      64.000us         0.00%     103.000us       6.867us            15  \n",
      "                  aten::slice         0.00%      31.000us         0.00%      45.000us       3.000us            15  \n",
      "                    aten::sub         0.02%     620.000us         0.03%     936.000us      31.200us            30  \n",
      "                    aten::div        30.27%     825.387ms        49.87%        1.360s      30.216ms            45  \n",
      "                    aten::add         0.00%     119.000us         0.01%     159.000us       2.650us            60  \n",
      "                aten::flatten         0.00%       2.000us         0.00%       2.000us       0.067us            30  \n",
      "                aten::one_hot         0.04%       1.091ms         0.11%       3.112ms     103.733us            30  \n",
      "                    aten::min         0.03%     758.000us         0.03%     762.000us      25.400us            30  \n",
      "                  aten::empty         0.00%       3.000us         0.00%       3.000us       0.033us            90  \n",
      "                  aten::fill_         0.00%       1.000us         0.00%       1.000us       0.017us            60  \n",
      "                    aten::max         0.00%      38.000us         0.00%      38.000us       1.267us            30  \n",
      "                  aten::zeros         0.02%     523.000us         0.02%     540.000us      18.000us            30  \n",
      "                  aten::zero_         0.00%      21.000us         0.00%      21.000us       0.700us            30  \n",
      "              aten::unsqueeze         0.00%      74.000us         0.00%      80.000us       2.667us            30  \n",
      "               aten::scatter_         0.02%     562.000us         0.02%     562.000us      18.733us            30  \n",
      "                 aten::hstack         0.01%     240.000us         0.03%     813.000us      27.100us            30  \n",
      "             aten::atleast_1d         0.01%     171.000us         0.01%     171.000us       5.700us            30  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.726s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(p.key_averages().table(row_limit=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "       aten::lift_fresh         0.03%       1.492ms         0.03%       1.492ms       0.001us       1618944  \n",
      "               aten::to        11.25%     638.390ms        81.60%        4.630s       2.860us       1618944  \n",
      "         aten::_to_copy        27.70%        1.572s        73.22%        4.154s       2.566us       1618944  \n",
      "    aten::empty_strided         0.15%       8.312ms         0.15%       8.312ms       0.005us       1618944  \n",
      "            aten::copy_        51.87%        2.943s        51.87%        2.943s       1.818us       1618944  \n",
      "          aten::detach_         0.30%      17.169ms         0.32%      18.002ms       0.011us       1618944  \n",
      "                detach_         0.02%     946.000us         0.02%     946.000us       0.001us       1618944  \n",
      "            aten::stack         1.47%      83.251ms         8.69%     492.803ms      56.618us          8704  \n",
      "              aten::cat         7.06%     400.483ms         7.21%     409.262ms      47.020us          8704  \n",
      "           aten::narrow         0.14%       8.215ms         0.15%       8.728ms       1.003us          8704  \n",
      "            aten::slice         0.01%     342.000us         0.01%     563.000us       0.065us          8704  \n",
      "       aten::as_strided         0.00%     222.000us         0.00%     222.000us       0.026us          8704  \n",
      "             aten::view         0.01%     290.000us         0.01%     290.000us       0.033us          8704  \n",
      "-----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 5.674s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WARNING - don't run this\n",
    "# Adding the profiler makes it take forever and consume an ungodly amount of RAM (python process goes to 50GB RAM)\n",
    "with torch.profiler.profile(on_trace_ready=torch.profiler.tensorboard_trace_handler(\"runs/dataload_profile\")) as p:\n",
    "  test_load_time(mydataset)\n",
    "# test_load_time(mydataloader)\n",
    "print(p.key_averages().table(row_limit=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataset = datasets.load_from_disk(\"dataset/hf_test/scripted_trajectories_50_2024-07-14_14-25-22.hf\").with_format(\"torch\")\n",
    "mydataset = mydataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg load time: 0.006466682863235474, std: 0.0017202445870103937\n"
     ]
    }
   ],
   "source": [
    "test_load_time(mydataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep the dataset in RAM\n",
    "Offers no speed up, so clearly not a disk read speed issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramdataset = datasets.load_from_disk(\"dataset/hf_test/scripted_trajectories_50_2024-07-14_14-25-22.hf\", keep_in_memory=True).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3023523e364db7b89d8672b79289ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ramdataset = ramdataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg load time: 0.005494078191121419, std: 0.0016571423263816668\n"
     ]
    }
   ],
   "source": [
    "test_load_time(ramdataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform on the fly\n",
    "Instead of using .map(), just apply the transform to each batch as we load it, each iteration, with set_transform.\n",
    "set_transform seems to override with_format(\"torch\") so we have to do the torch conversion ourselves\n",
    "\n",
    "This is much quicker. .map is broken!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    \"\"\"\n",
    "    Take a batch of data and put it in a suitable tensor format for the model\n",
    "    \"\"\"\n",
    "    bounds_centre = torch.tensor([0]*6)\n",
    "    bounds_range = torch.tensor([12]*6)\n",
    "    out = {}\n",
    "    # start = time.time()\n",
    "    def normalize_qpos(qpos):\n",
    "        return (qpos - bounds_centre) / bounds_range + 0.5\n",
    "    \n",
    "    observation_qpos_normalised = normalize_qpos(torch.tensor(batch[\"observation.state.qpos\"], dtype=torch.float32))\n",
    "    observation_gripper = embed_gripper(torch.tensor(batch[\"observation.state.gripper\"], dtype=int)).to(torch.float32)\n",
    "    out[\"preprocessed.observation.state\"] = torch.hstack((observation_qpos_normalised, observation_gripper))\n",
    "\n",
    "    action_qpos_normalised = normalize_qpos(torch.tensor(batch[\"action.qpos\"], dtype=torch.float32))\n",
    "    action_gripper = embed_gripper(torch.tensor(batch[\"action.gripper\"], dtype=int)).to(torch.float32)\n",
    "    out[\"preprocessed.action.state\"] = torch.hstack((action_qpos_normalised, action_gripper))\n",
    "    \n",
    "    # Convert to float32 with image from channel first in [0,255]\n",
    "    tf = torchvision.transforms.ToTensor()\n",
    "    out[\"preprocessed.observation.image\"] = torch.stack([tf(x) for x in batch[\"observation.pixels\"]])\n",
    "\n",
    "\n",
    "    return out\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "def preprocess_on_device(batch):\n",
    "    \"\"\"\n",
    "    Take a batch of data and put it in a suitable tensor format for the model\n",
    "    \"\"\"\n",
    "    bounds_centre = torch.tensor([0]*6).to(device)\n",
    "    bounds_range = torch.tensor([12]*6).to(device)\n",
    "    out = {}\n",
    "    # start = time.time()\n",
    "    def normalize_qpos(qpos):\n",
    "        return (qpos - bounds_centre) / bounds_range + 0.5\n",
    "    \n",
    "    observation_qpos_normalised = normalize_qpos(torch.tensor(batch[\"observation.state.qpos\"], dtype=torch.float32).to(device))\n",
    "    observation_gripper = embed_gripper(torch.tensor(batch[\"observation.state.gripper\"], dtype=int).to(device)).to(torch.float32)\n",
    "    out[\"preprocessed.observation.state\"] = torch.hstack((observation_qpos_normalised, observation_gripper))\n",
    "\n",
    "    action_qpos_normalised = normalize_qpos(torch.tensor(batch[\"action.qpos\"], dtype=torch.float32).to(device))\n",
    "    action_gripper = embed_gripper(torch.tensor(batch[\"action.gripper\"], dtype=int).to(device)).to(torch.float32)\n",
    "    out[\"preprocessed.action.state\"] = torch.hstack((action_qpos_normalised, action_gripper))\n",
    "    \n",
    "    # Create tensor stack, move to GPU, normalise\n",
    "    tf = torchvision.transforms.PILToTensor()\n",
    "    out[\"preprocessed.observation.image\"] = torch.stack([tf(x) for x in batch[\"observation.pixels\"]], dim=0).to(device)\n",
    "    out[\"preprocessed.observation.image\"] = out[\"preprocessed.observation.image\"] / 255\n",
    "\n",
    "\n",
    "    return out\n",
    "\n",
    "def transform_function(batch):\n",
    "    batch[\"preprocessed.observation.image\"] = batch[\"observation.pixels\"]/ 255\n",
    "    return batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataset = datasets.load_from_disk(\"dataset/hf_test/scripted_trajectories_50_2024-07-14_14-25-22.hf\", keep_in_memory=True)\n",
    "# mydataset = mydataset.map(preprocess_function, batched=True)\n",
    "mydataset.set_transform(preprocess_function)\n",
    "# mydataset.set_transform(preprocess_on_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg load time: 0.001014552625020345, std: 0.0002771132183452647\n"
     ]
    }
   ],
   "source": [
    "test_load_time(mydataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multithreading doesn't work, probably a jupyter issue\n",
    "mydataloader = DataLoader(mydataset, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg load time: 0.2979206756009894, std: 0.02904730607116912\n"
     ]
    }
   ],
   "source": [
    "test_load_time(mydataloader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
