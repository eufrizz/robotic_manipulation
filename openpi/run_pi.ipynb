{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lerobot\n",
    "from lerobot.policies.factory import make_policy\n",
    "from lerobot.configs.train import TrainPipelineConfig, PreTrainedConfig\n",
    "from lerobot.policies.pi05 import (  # noqa: E402\n",
    "    PI05Config,\n",
    "    PI05Policy,\n",
    "    make_pi05_pre_post_processors,  # noqa: E402\n",
    ")\n",
    "import lerobot.policies.pi05\n",
    "from lerobot.configs.default import DatasetConfig, EvalConfig, WandBConfig\n",
    "from pprint import pprint\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766853ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "if \"policy\" in locals():\n",
    "    del policy\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec365346",
   "metadata": {},
   "source": [
    "# Mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf8f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PI05Config(max_action_dim=7, max_state_dim=14, dtype=\"bfloat16\", device=\"cuda\")\n",
    "\n",
    "    # Set up input_features and output_features in the config\n",
    "from lerobot.configs.types import FeatureType, PolicyFeature\n",
    "\n",
    "config.input_features = {\n",
    "    \"observation.state\": PolicyFeature(\n",
    "        type=FeatureType.STATE,\n",
    "        shape=(14,),\n",
    "    ),\n",
    "    \"observation.images.base_1_rgb\": PolicyFeature(\n",
    "        type=FeatureType.VISUAL,\n",
    "        shape=(3, 224, 224),\n",
    "    ),\n",
    "}\n",
    "\n",
    "config.output_features = {\n",
    "    \"action\": PolicyFeature(\n",
    "        type=FeatureType.ACTION,\n",
    "        shape=(7,),\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a61e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PI05Policy(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49225aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "device = \"cuda\"\n",
    "dataset_stats = {\n",
    "        \"observation.state\": {\n",
    "            \"mean\": torch.zeros(14),\n",
    "            \"std\": torch.ones(14),\n",
    "            \"min\": torch.zeros(14),\n",
    "            \"max\": torch.ones(14),\n",
    "            \"q01\": torch.zeros(14),\n",
    "            \"q99\": torch.ones(14),\n",
    "        },\n",
    "        \"action\": {\n",
    "            \"mean\": torch.zeros(7),\n",
    "            \"std\": torch.ones(7),\n",
    "            \"min\": torch.zeros(7),\n",
    "            \"max\": torch.ones(7),\n",
    "            \"q01\": torch.zeros(7),\n",
    "            \"q99\": torch.ones(7),\n",
    "        },\n",
    "        \"observation.images.base_1_rgb\": {\n",
    "            \"mean\": torch.zeros(3, 224, 224),\n",
    "            \"std\": torch.ones(3, 224, 224),\n",
    "            \"q01\": torch.zeros(3, 224, 224),\n",
    "            \"q99\": torch.ones(3, 224, 224),\n",
    "        },\n",
    "    }\n",
    "preprocessor, postprocessor = make_pi05_pre_post_processors(config=config, dataset_stats=dataset_stats)\n",
    "batch = {\n",
    "        \"observation.state\": torch.randn(batch_size, 14, dtype=torch.float32, device=device),\n",
    "        \"action\": torch.randn(batch_size, config.chunk_size, 7, dtype=torch.float32, device=device),\n",
    "        \"observation.images.base_1_rgb\": torch.rand(\n",
    "            batch_size, 3, 224, 224, dtype=torch.float32, device=device\n",
    "        ),  # Use rand for [0,1] range\n",
    "        \"task\": [\"Pick up the object\"] * batch_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f154d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = preprocessor(batch)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94475e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = policy.select_action(input)\n",
    "output = postprocessor(action)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0baf0bc",
   "metadata": {},
   "source": [
    "# Xarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco\n",
    "from pathlib import Path\n",
    "import gym_lite6\n",
    "import gymnasium as gym\n",
    "import gym_lite6.env, gym_lite6.scripted_policy, gym_lite6.pickup_task\n",
    "import mediapy as media\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "reload(gym_lite6.env)\n",
    "reload(gym_lite6.utils)\n",
    "reload(gym_lite6.scripted_policy)\n",
    "reload(gym_lite6.pickup_task)\n",
    "\n",
    "# task = gym_lite6.pickup_task.GraspTask('gripper_left_finger', 'gripper_right_finger', 'box', 'floor')\n",
    "task = gym_lite6.pickup_task.GraspAndLiftTask('gripper_left_finger', 'gripper_right_finger', 'box', 'floor')\n",
    "\n",
    "env = gym.make(\n",
    "    \"UfactoryCubePickup-v0\",\n",
    "    task=task,\n",
    "    obs_type=\"pixels_state\",\n",
    "    max_episode_steps=500,\n",
    "    visualization_width=320,\n",
    "    visualization_height=240,\n",
    "    render_fps=30,\n",
    "    joint_noise_magnitude=0.1\n",
    ")\n",
    "\n",
    "\n",
    "observation, info = env.reset()\n",
    "media.show_image(env.unwrapped.render(camera=\"side_cam\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
    "\n",
    "dataset_path= \"/media/ssd/eugene/robotic_manipulation/lerobot_tests/datasets/lite6_record_scripted_250622\"\n",
    "dataset_meta = LeRobotDatasetMetadata(dataset_path)\n",
    "dataset_meta.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db820a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_meta.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8abcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PI05Config(max_action_dim=7, max_state_dim=7, dtype=\"bfloat16\", device=\"cuda\")\n",
    "\n",
    "    # Set up input_features and output_features in the config\n",
    "from lerobot.configs.types import FeatureType, PolicyFeature\n",
    "\n",
    "config.input_features = {\n",
    "    \"observation.state\": PolicyFeature(\n",
    "        type=FeatureType.STATE,\n",
    "        shape=(7,),\n",
    "    ),\n",
    "    \"observation.images.side\": PolicyFeature(\n",
    "        type=FeatureType.VISUAL,\n",
    "        shape=(240, 320, 3),\n",
    "    ),\n",
    "    \"observation.images.gripper\": PolicyFeature(\n",
    "        type=FeatureType.VISUAL,\n",
    "        shape=(240, 320, 3),\n",
    "    ),\n",
    "}\n",
    "\n",
    "config.output_features = {\n",
    "    \"action\": PolicyFeature(\n",
    "        type=FeatureType.ACTION,\n",
    "        shape=(7,),\n",
    "    ),\n",
    "}\n",
    "\n",
    "preprocessor, postprocessor = make_pi05_pre_post_processors(config=config, dataset_stats=dataset_meta.stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a99ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_observation, info = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd26479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_torch_obs(numpy_observation):\n",
    "    observation = {}\n",
    "    observation[\"observation.state\"] = torch.from_numpy(np.float32(np.hstack((numpy_observation[\"state\"][\"qpos\"], numpy_observation[\"state\"][\"gripper\"])))).unsqueeze(0).to(config.device)\n",
    "    # DIVIDE BY 255\n",
    "    observation[\"observation.images.side\"] = torch.from_numpy(numpy_observation['pixels']['side']).permute((2,0,1)).unsqueeze(0).to(config.device)/255\n",
    "    observation[\"observation.images.gripper\"] = torch.from_numpy(numpy_observation['pixels']['gripper']).permute((2,0,1)).unsqueeze(0).to(config.device)/255\n",
    "    return observation\n",
    "observation = numpy_to_torch_obs(numpy_observation)\n",
    "observation[\"task\"] = [\"Pick up the red cube\"]\n",
    "observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PI05Policy(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b862f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = numpy_to_torch_obs(numpy_observation)\n",
    "observation[\"task\"] = [\"Pick up the red cube\"]\n",
    "observation = preprocessor(observation)\n",
    "policy.select_action(observation)\n",
    "postprocessor(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.reset()\n",
    "policy.eval()\n",
    "numpy_observation, info = env.reset()\n",
    "rewards = []\n",
    "frames = [numpy_observation[\"pixels\"][\"side\"].squeeze()]\n",
    "done = False\n",
    "observation = {}\n",
    "step = 0\n",
    "\n",
    "ep_dict = {\"action.qpos\": [], \"action.gripper\": [], \"observation.state.qpos\": [], \"observation.state.qvel\": [], \"observation.state.gripper\": [], \"observation.images.side\": [], \"observation.images.gripper\": [], \"reward\": [], \"timestamp\": [], \"frame_index\": [],}\n",
    "while not done:\n",
    "    observation = numpy_to_torch_obs(numpy_observation)\n",
    "    observation[\"task\"] = [\"Pick up the red cube\"]\n",
    "    observation = preprocessor(observation)\n",
    "    with torch.inference_mode():\n",
    "        action = policy.select_action(observation)\n",
    "        action = postprocessor(action)[0]\n",
    "    action = {\"qpos\": action[:env.unwrapped.dof], \"gripper\": round(np.clip(action[-1].item(), -1, 1))}\n",
    "    numpy_observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    rewards.append(reward)\n",
    "    frames.append(numpy_observation[\"pixels\"][\"side\"].squeeze())\n",
    "\n",
    "    ep_dict[\"action.qpos\"].append(action[\"qpos\"])\n",
    "    ep_dict[\"action.gripper\"].append(action[\"gripper\"])\n",
    "    ep_dict[\"observation.state.qpos\"].append(numpy_observation[\"state\"][\"qpos\"])\n",
    "    ep_dict[\"observation.state.qvel\"].append(numpy_observation[\"state\"][\"qvel\"])\n",
    "    ep_dict[\"observation.state.gripper\"].append(numpy_observation[\"state\"][\"gripper\"])\n",
    "    ep_dict[\"observation.images.side\"].append(numpy_observation[\"pixels\"][\"side\"])\n",
    "    ep_dict[\"observation.images.gripper\"].append(numpy_observation[\"pixels\"][\"gripper\"])\n",
    "    ep_dict[\"reward\"].append(reward)\n",
    "    ep_dict[\"timestamp\"].append(env.unwrapped.data.time)\n",
    "    ep_dict[\"frame_index\"].append(step)\n",
    "\n",
    "    done = terminated | truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58da871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapy as media\n",
    "\n",
    "media.show_video(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c332b95",
   "metadata": {},
   "source": [
    "# Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ffc633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
    "from lerobot.datasets.factory import make_dataset\n",
    "from lerobot.policies.factory import make_policy\n",
    "\n",
    "dataset_path= \"/media/ssd/eugene/robotic_manipulation/lerobot_tests/datasets/lite6_record_scripted_250622\"\n",
    "dataset_name = dataset_path.split('/')[-1]\n",
    "dataset_meta = LeRobotDatasetMetadata(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664ea6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lerobot.policies.pretrained import PreTrainedPolicy\n",
    "\n",
    "policy_config = PI05Config(\n",
    "    max_action_dim=7,\n",
    "    max_state_dim=7,\n",
    "    dtype=\"bfloat16\",\n",
    "    device=\"cuda\",\n",
    "    scheduler_decay_steps=3000,\n",
    "    compile_model=True,\n",
    "    gradient_checkpointing=True,\n",
    "    push_to_hub=False\n",
    "    )\n",
    "\n",
    "    # Set up input_features and output_features in the config\n",
    "from lerobot.configs.types import FeatureType, PolicyFeature\n",
    "\n",
    "policy_config.input_features = {\n",
    "    \"observation.state\": PolicyFeature(\n",
    "        type=FeatureType.STATE,\n",
    "        shape=(7,),\n",
    "    ),\n",
    "    \"observation.images.side\": PolicyFeature(\n",
    "        type=FeatureType.VISUAL,\n",
    "        shape=(240, 320, 3),\n",
    "    ),\n",
    "    \"observation.images.gripper\": PolicyFeature(\n",
    "        type=FeatureType.VISUAL,\n",
    "        shape=(240, 320, 3),\n",
    "    ),\n",
    "}\n",
    "\n",
    "policy_config.output_features = {\n",
    "    \"action\": PolicyFeature(\n",
    "        type=FeatureType.ACTION,\n",
    "        shape=(7,),\n",
    "    ),\n",
    "}\n",
    "\n",
    "preprocessor, postprocessor = make_pi05_pre_post_processors(config=policy_config, dataset_stats=dataset_meta.stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e4f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "job_name = \"pi05_training\"\n",
    "train_dir = f\"{now:%Y-%m-%d}/{now:%H-%M-%S}_{job_name}\"\n",
    "output_dir = Path(\"outputs\") / dataset_name / train_dir\n",
    "\n",
    "cfg = TrainPipelineConfig(\n",
    "    dataset=DatasetConfig(dataset_path),\n",
    "    policy=policy_config,\n",
    "    batch_size=1,\n",
    "    steps=3000,\n",
    "    job_name=job_name,\n",
    "    num_workers=2,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "cfg.validate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d678d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: use make_dataset to automatically handle action_delta_timestamps\n",
    "\n",
    "dataset = make_dataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089222f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=cfg.num_workers,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True, # TODO: episode aware sampler?\n",
    "    sampler=None,\n",
    "    pin_memory=False, #no need on unified mem cfg.policy.device == \"cuda\",\n",
    "    drop_last=False,\n",
    "    prefetch_factor=2 if cfg.num_workers > 0 else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad6046",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = make_policy(cfg.policy, dataset.meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86357db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.optim.factory import make_optimizer_and_scheduler\n",
    "optimizer, lr_scheduler = make_optimizer_and_scheduler(cfg, policy)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c5043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.datasets.utils import cycle\n",
    "from lerobot.utils.logging_utils import AverageMeter, MetricsTracker\n",
    "\n",
    "step = 0  # number of policy updates (forward + backward + optim)\n",
    "dl_iter = cycle(dataloader)\n",
    "\n",
    "\n",
    "train_metrics = {\n",
    "        \"loss\": AverageMeter(\"loss\", \":.3f\"),\n",
    "        \"grad_norm\": AverageMeter(\"grdn\", \":.3f\"),\n",
    "        \"lr\": AverageMeter(\"lr\", \":0.1e\"),\n",
    "        \"update_s\": AverageMeter(\"updt_s\", \":.3f\"),\n",
    "        \"dataloading_s\": AverageMeter(\"data_s\", \":.3f\"),\n",
    "    }\n",
    "\n",
    "train_tracker = MetricsTracker(\n",
    "        cfg.batch_size,\n",
    "        dataset.num_frames,\n",
    "        dataset.num_episodes,\n",
    "        train_metrics,\n",
    "        initial_step=step,\n",
    "        accelerator=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Any\n",
    "from torch.optim import Optimizer\n",
    "import logging\n",
    "from lerobot.utils.train_utils import (\n",
    "    get_step_checkpoint_dir,\n",
    "    get_step_identifier,\n",
    "    load_training_state,\n",
    "    save_checkpoint,\n",
    "    update_last_checkpoint,\n",
    ")\n",
    "\n",
    "def update_policy(\n",
    "    train_metrics: MetricsTracker,\n",
    "    policy,\n",
    "    batch: Any,\n",
    "    optimizer: Optimizer,\n",
    "    grad_clip_norm: float,\n",
    "    lr_scheduler,\n",
    "    lock=None,\n",
    ") -> tuple[MetricsTracker, dict]:\n",
    "    start_time = time.perf_counter()\n",
    "    policy.train()\n",
    "\n",
    "    # Let accelerator handle mixed precision\n",
    "    # with accelerator.autocast():\n",
    "    #     loss, output_dict = policy.forward(batch)\n",
    "    #     # TODO(rcadene): policy.unnormalize_outputs(out_dict)\n",
    "\n",
    "    # loss, output_dict = policy.forward(batch)\n",
    "    # loss.backward()\n",
    "    loss = 1\n",
    "    output_dict={}\n",
    "\n",
    "    # Clip gradients if specified\n",
    "\n",
    "    # grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "    #     policy.parameters(), float(\"inf\"), error_if_nonfinite=False\n",
    "    # )\n",
    "\n",
    "    # optimizer.step()\n",
    "    # optimizer.zero_grad()\n",
    "\n",
    "    # Step through pytorch scheduler at every batch instead of epoch\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    # Update internal buffers if policy has update method\n",
    "    # if has_method(accelerator.unwrap_model(policy, keep_fp32_wrapper=True), \"update\"):\n",
    "    #     accelerator.unwrap_model(policy, keep_fp32_wrapper=True).update()\n",
    "\n",
    "    train_metrics.loss = loss.item()\n",
    "    train_metrics.grad_norm = grad_norm.item()\n",
    "    train_metrics.lr = optimizer.param_groups[0][\"lr\"]\n",
    "    train_metrics.update_s = time.perf_counter() - start_time\n",
    "    return train_metrics, output_dict\n",
    "\n",
    "def create_state(batch):\n",
    "    batch[\"observation.state\"] = torch.cat((batch[\"observation.state.qpos\"], batch[\"observation.state.gripper\"].unsqueeze(1)), dim=1)\n",
    "\n",
    "for _ in range(step, cfg.steps):\n",
    "    start_time = time.perf_counter()\n",
    "    batch = next(dl_iter)\n",
    "    create_state(batch)\n",
    "    batch = preprocessor(batch)\n",
    "\n",
    "    train_tracker.dataloading_s = time.perf_counter() - start_time\n",
    "\n",
    "    train_tracker, output_dict = update_policy(\n",
    "                train_tracker,\n",
    "                policy,\n",
    "                batch,\n",
    "                optimizer,\n",
    "                0,\n",
    "                lr_scheduler=lr_scheduler,\n",
    "            )\n",
    "    step += 1\n",
    "    train_tracker.step()\n",
    "    logging.info(train_tracker)\n",
    "\n",
    "    is_log_step = cfg.log_freq > 0 and step % cfg.log_freq == 0\n",
    "    is_saving_step = step % cfg.save_freq == 0 or step == cfg.steps\n",
    "    is_eval_step = cfg.eval_freq > 0 and step % cfg.eval_freq == 0\n",
    "\n",
    "    if is_log_step:\n",
    "            logging.info(train_tracker)\n",
    "            train_tracker.reset_averages()\n",
    "\n",
    "    if cfg.save_checkpoint and is_saving_step:\n",
    "        logging.info(f\"Checkpoint policy after step {step}\")\n",
    "        checkpoint_dir = get_step_checkpoint_dir(cfg.output_dir, cfg.steps, step)\n",
    "        save_checkpoint(\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            step=step,\n",
    "            cfg=cfg,\n",
    "            policy=policy,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=lr_scheduler,\n",
    "            preprocessor=preprocessor,\n",
    "            postprocessor=postprocessor,\n",
    "        )\n",
    "        update_last_checkpoint(checkpoint_dir)\n",
    "\n",
    "\n",
    "        # if cfg.env and is_eval_step:\n",
    "        #     step_id = get_step_identifier(step, cfg.steps)\n",
    "        #     logging.info(f\"Eval policy at step {step}\")\n",
    "        #     with torch.no_grad():\n",
    "        #         eval_info = eval_policy_all(\n",
    "        #             envs=eval_env,  # dict[suite][task_id] -> vec_env\n",
    "        #             policy=accelerator.unwrap_model(policy),\n",
    "        #             preprocessor=preprocessor,\n",
    "        #             postprocessor=postprocessor,\n",
    "        #             n_episodes=cfg.eval.n_episodes,\n",
    "        #             videos_dir=cfg.output_dir / \"eval\" / f\"videos_step_{step_id}\",\n",
    "        #             max_episodes_rendered=4,\n",
    "        #             start_seed=cfg.seed,\n",
    "        #             max_parallel_tasks=cfg.env.max_parallel_tasks,\n",
    "        #         )\n",
    "        #     # overall metrics (suite-agnostic)\n",
    "        #     aggregated = eval_info[\"overall\"]\n",
    "\n",
    "        #     # optional: per-suite logging\n",
    "        #     for suite, suite_info in eval_info.items():\n",
    "        #         logging.info(\"Suite %s aggregated: %s\", suite, suite_info)\n",
    "\n",
    "        #     # meters/tracker\n",
    "        #     eval_metrics = {\n",
    "        #         \"avg_sum_reward\": AverageMeter(\"âˆ‘rwrd\", \":.3f\"),\n",
    "        #         \"pc_success\": AverageMeter(\"success\", \":.1f\"),\n",
    "        #         \"eval_s\": AverageMeter(\"eval_s\", \":.3f\"),\n",
    "        #     }\n",
    "        #     eval_tracker = MetricsTracker(\n",
    "        #         cfg.batch_size,\n",
    "        #         dataset.num_frames,\n",
    "        #         dataset.num_episodes,\n",
    "        #         eval_metrics,\n",
    "        #         initial_step=step,\n",
    "        #     )\n",
    "        #     eval_tracker.eval_s = aggregated.pop(\"eval_s\")\n",
    "        #     eval_tracker.avg_sum_reward = aggregated.pop(\"avg_sum_reward\")\n",
    "        #     eval_tracker.pc_success = aggregated.pop(\"pc_success\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec6ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac1a9df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lerobot_venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
