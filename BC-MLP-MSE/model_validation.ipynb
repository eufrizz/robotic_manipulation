{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import mediapy as media\n",
    "import torch\n",
    "# torch.multiprocessing.set_start_method('spawn')\n",
    "import gym_lite6.env, gym_lite6.pickup_task, gym_lite6.utils, gym_lite6.policies\n",
    "%env MUJOCO_GL=egl # Had to export this before starting jupyter server\n",
    "# import mujoco\n",
    "\n",
    "from importlib import reload\n",
    "import gym_lite6.policies.mlp\n",
    "reload(gym_lite6.policies.mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = torch.load(\"BC-MLP-MSE/ckpts/lite6_grasp_h64_64_64_2024-09-02_00-11-31/epoch_20.pt\")\n",
    "start_epoch = checkpoint[\"epoch\"]\n",
    "step = checkpoint[\"step\"]\n",
    "params = checkpoint[\"params\"]\n",
    "\n",
    "params[\"device\"] = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy = gym_lite6.policies.mlp.MLPPolicy([64, 64, 64]).to(params[\"device\"])\n",
    "policy.eval()\n",
    "policy.load_state_dict(checkpoint[\"policy_state_dict\"])\n",
    "\n",
    "print(f\"Loaded checkpoint at epoch {start_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = gym_lite6.pickup_task.GraspTask('gripper_left_finger', 'gripper_right_finger', 'box', 'floor')\n",
    "\n",
    "env = gym.make(\n",
    "    \"UfactoryCubePickup-v0\",\n",
    "    task=task,\n",
    "    obs_type=\"pixels_state\",\n",
    "    max_episode_steps=150,\n",
    "    visualization_width=320,\n",
    "    visualization_height=240\n",
    ")\n",
    "qpos0 = np.array([0, 0.541, 1.49 , 2.961, 0.596, 0.203])\n",
    "box_pos0 = np.array([0.2, 0, 0.0])\n",
    "box_quat0 = None\n",
    "observation, info = env.reset(qpos=qpos0, box_pos=box_pos0, box_quat=box_quat0)\n",
    "media.show_image(env.unwrapped.render(camera=\"side_cam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gym_lite6.policies.mlp.Interface(params)\n",
    "avg_reward = 0\n",
    "observation, info = env.reset(qpos=qpos0, box_pos=box_pos0, box_quat=box_quat0)\n",
    "\n",
    "# Prepare to collect every rewards and all the frames of the episode,\n",
    "# from initial state to final state.\n",
    "rewards = []\n",
    "frames = []\n",
    "action = {}\n",
    "\n",
    "ep_dict = {\"action.qpos\": [], \"action.gripper\": [], \"observation.state.qpos\": [], \"observation.state.qvel\": [], \"observation.state.gripper\": [], \"observation.pixels.side\": [], \"observation.pixels.gripper\": [], \"reward\": [], \"timestamp\": [], \"frame_index\": [],}\n",
    "\n",
    "# Render frame of the initial state\n",
    "frames.append(env.render())\n",
    "\n",
    "step = 0\n",
    "done = False\n",
    "# Will get truncated at max_episode_steps\n",
    "while not done:\n",
    "  # Prepare observation for the policy running in Pytorch\n",
    "  # Get qpos in range (-1, 1), gripper is already in range (-1, 1)\n",
    "  qpos = torch.from_numpy(observation[\"state\"][\"qpos\"]).unsqueeze(0)\n",
    "  gripper = trainer.embed_gripper(torch.tensor(observation[\"state\"][\"gripper\"])).unsqueeze(0)\n",
    "  if trainer.params[\"normalize_qpos\"]:\n",
    "    qpos = trainer.normalize_qpos(qpos)\n",
    "  state = torch.hstack((qpos, gripper))\n",
    "  image_side = torch.from_numpy(observation[\"pixels\"][\"side\"]).permute(2, 0, 1).unsqueeze(0) / 255\n",
    "  image_gripper = torch.from_numpy(observation[\"pixels\"][\"gripper\"]).permute(2, 0, 1).unsqueeze(0) / 255\n",
    "  \n",
    "  # Convert to float32 with image from channel first in [0,255]\n",
    "  # to channel last in [0,1]\n",
    "  state = state.to(torch.float32)\n",
    "\n",
    "  # Send data tensors from CPU to GPU\n",
    "  state = state.to(trainer.params[\"device\"], non_blocking=True)\n",
    "  image_side = image_side.to(trainer.params[\"device\"], non_blocking=True)\n",
    "  image_gripper = image_gripper.to(trainer.params[\"device\"], non_blocking=True)\n",
    "\n",
    "  # Predict the next action with respect to the current observation\n",
    "  with torch.inference_mode():\n",
    "    raw_action = policy.predict(state, image_side, image_gripper).to(\"cpu\")\n",
    "  \n",
    "  action[\"qpos\"] = raw_action[:, :6]\n",
    "  if trainer.params[\"normalize_qpos\"]:\n",
    "    action[\"qpos\"] = trainer.unnormalize_qpos(action[\"qpos\"])\n",
    "  \n",
    "  action[\"qpos\"] = action[\"qpos\"].flatten().numpy()\n",
    "  action[\"gripper\"] = trainer.decode_gripper(raw_action[:, 6:8]).item()\n",
    "\n",
    "  # Step through the environment and receive a new observation\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "  # Keep track of all the rewards and frames\n",
    "  rewards.append(reward)\n",
    "  frames.append(env.render())\n",
    "\n",
    "  ep_dict[\"action.qpos\"].append(action[\"qpos\"])\n",
    "  ep_dict[\"action.gripper\"].append(action[\"gripper\"])\n",
    "  ep_dict[\"observation.state.qpos\"].append(observation[\"state\"][\"qpos\"])\n",
    "  ep_dict[\"observation.state.qvel\"].append(observation[\"state\"][\"qvel\"])\n",
    "  ep_dict[\"observation.state.gripper\"].append(observation[\"state\"][\"gripper\"])\n",
    "  ep_dict[\"observation.pixels.side\"].append(observation[\"pixels\"][\"side\"])\n",
    "  ep_dict[\"observation.pixels.gripper\"].append(observation[\"pixels\"][\"gripper\"])\n",
    "  ep_dict[\"reward\"].append(reward)\n",
    "  ep_dict[\"timestamp\"].append(env.unwrapped.data.time)\n",
    "  ep_dict[\"frame_index\"].append(step)\n",
    "\n",
    "  # The rollout is considered done when the success state is reach (i.e. terminated is True),\n",
    "  # or the maximum number of iterations is reached (i.e. truncated is True)\n",
    "  done = terminated | truncated | done\n",
    "  step += 1\n",
    "\n",
    "avg_reward = np.mean(rewards)\n",
    "print(params)\n",
    "print(f\"Average reward: {avg_reward}\")\n",
    "media.show_videos([ep_dict[\"observation.pixels.side\"], ep_dict[\"observation.pixels.gripper\"]], fps=env.metadata[\"render_fps\"])\n",
    "gym_lite6.utils.plot_dict_of_arrays(ep_dict, \"timestamp\", keys=[\"action.qpos\", \"observation.state.qpos\", \"observation.state.qvel\", \"action.gripper\", \"reward\"], sharey=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward = np.mean(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
