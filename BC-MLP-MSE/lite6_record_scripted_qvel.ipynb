{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import mediapy as media\n",
    "import torch\n",
    "import gym_lite6.env, gym_lite6.scripted_policy, gym_lite6.pickup_task\n",
    "# Had to export this before starting jupyter server\n",
    "%env MUJOCO_GL=egl \n",
    "import mujoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "reload(gym_lite6.env)\n",
    "reload(gym_lite6.utils)\n",
    "reload(gym_lite6.scripted_policy)\n",
    "reload(gym_lite6.pickup_task)\n",
    "\n",
    "task = gym_lite6.pickup_task.GraspTask('gripper_left_finger', 'gripper_right_finger', 'box', 'floor')\n",
    "# task = gym_lite6.pickup_task.GraspAndLiftTask('gripper_left_finger', 'gripper_right_finger', 'box', 'floor')\n",
    "\n",
    "env = gym.make(\n",
    "    \"UfactoryCubePickup-v0\",\n",
    "    task=task,\n",
    "    obs_type=\"pixels_state\",\n",
    "    action_type=\"qvel\",\n",
    "    max_episode_steps=300,\n",
    "    visualization_width=320,\n",
    "    visualization_height=240\n",
    ")\n",
    "\n",
    "observation, info = env.reset()\n",
    "media.show_image(env.unwrapped.render(camera=\"side_cam\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qpos = None\n",
    "box_pos = None\n",
    "box_quat = None\n",
    "\n",
    "# qpos0 = np.array([0, 0.541, 1.49 , 2.961, 0.596, 0.203])\n",
    "# box_pos0 = np.array([0.2, 0, 0.0])\n",
    "# box_quat0 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a scripted rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a scripted rollout\n",
    "\n",
    "policy = gym_lite6.scripted_policy.GraspPolicy(env, 'end_effector', 'box', 'gripper_left_finger', 'gripper_right_finger', max_vel=0.2)\n",
    "# policy = gym_lite6.scripted_policy.GraspAndLiftPolicy(env, 'end_effector', 'box', 'gripper_left_finger', 'gripper_right_finger', max_vel=0.2)\n",
    "\n",
    "# Reset the policy and environmens to prepare for rollout\n",
    "policy.reset()\n",
    "observation, info = env.reset(seed=69, qpos=None, box_pos=None, box_quat=None)\n",
    "\n",
    "action = {}\n",
    "Kp = 0.6\n",
    "# Ki = 0\n",
    "# i_bounds = [-1,1]\n",
    "# i_error = np.zeros(env.unwrapped.dof)\n",
    "\n",
    "frames = []\n",
    "# Render frame of the initial state\n",
    "frames.append(env.render())\n",
    "\n",
    "step = 0\n",
    "done = False\n",
    "\n",
    "ep_dict = {\"action.qpos\": [], \"action.qvel\": [], \"action.gripper\": [], \"observation.state.qpos\": [], \"observation.state.qvel\": [], \"observation.state.gripper\": [], \"observation.pixels.side\": [], \"observation.pixels.gripper\": [], \"reward\": [], \"timestamp\": [], \"frame_index\": [],}\n",
    "\n",
    "while not done:\n",
    "  action = policy(env.unwrapped.model, env.unwrapped.data, observation, info)\n",
    "  dq = action[\"qpos\"] - observation[\"state\"][\"qpos\"]\n",
    "  # Optional integral error\n",
    "  # for i in range(env.unwrapped.dof):\n",
    "  #   if i_error[i] > i_bounds[0] and i_error[i] < i_bounds[1]:\n",
    "  #     i_error[i] += dq[i]/env.metadata[\"render_fps\"]\n",
    "  # print(f\"dq: {dq}, ierr: {i_error}\")\n",
    "  \n",
    "  action[\"qvel\"] = Kp * dq * env.metadata[\"render_fps\"]\n",
    "  # + Ki*i_error\n",
    "\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "  ep_dict[\"action.qpos\"].append(action[\"qpos\"])\n",
    "  ep_dict[\"action.qvel\"].append(action[\"qvel\"])\n",
    "  ep_dict[\"action.gripper\"].append(action[\"gripper\"])\n",
    "  ep_dict[\"observation.state.qpos\"].append(observation[\"state\"][\"qpos\"])\n",
    "  ep_dict[\"observation.state.qvel\"].append(observation[\"state\"][\"qvel\"])\n",
    "  ep_dict[\"observation.state.gripper\"].append(observation[\"state\"][\"gripper\"])\n",
    "  ep_dict[\"observation.pixels.side\"].append(observation[\"pixels\"][\"side\"])\n",
    "  ep_dict[\"observation.pixels.gripper\"].append(observation[\"pixels\"][\"gripper\"])\n",
    "  ep_dict[\"reward\"].append(reward)\n",
    "  ep_dict[\"timestamp\"].append(env.unwrapped.data.time)\n",
    "  ep_dict[\"frame_index\"].append(step)\n",
    "\n",
    "  if (policy.done):\n",
    "      terminated = True\n",
    "  print(f\"{step=} {reward=} {terminated=}\")\n",
    "\n",
    "  # The rollout is considered done when the success state is reach (i.e. terminated is True),\n",
    "  # or the maximum number of iterations is reached (i.e. truncated is True)\n",
    "  done = terminated | truncated | done\n",
    "  step += 1\n",
    "\n",
    "if terminated:\n",
    "  print(\"Success!\")\n",
    "else:\n",
    "  print(f\"Failure! Reached {policy.stage}\")\n",
    "\n",
    "media.show_video(ep_dict[\"observation.pixels.side\"], fps=env.metadata[\"render_fps\"])\n",
    "media.show_video(ep_dict[\"observation.pixels.gripper\"], fps=env.metadata[\"render_fps\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ep_dict.keys())\n",
    "\n",
    "gym_lite6.utils.plot_dict_of_arrays(ep_dict, \"timestamp\", keys=[\"action.qpos\", \"action.qvel\", \"observation.state.qpos\", \"observation.state.qvel\", \"observation.state.gripper\", \"reward\"], sharey=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record to HF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lerobot.common.datasets.push_dataset_to_hub.utils\n",
    "from datasets import Dataset, Features, Image, Sequence, Value\n",
    "from lerobot.common.datasets.utils import (\n",
    "    hf_transform_to_torch,\n",
    ")\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def record_episodes_to_hf(env, policy, dataset_dir, num_ep=1, num_frames=300):\n",
    "  features = {}\n",
    "  num_readings = num_ep*num_frames\n",
    "  features[\"observation.pixels.side\"] = Image()\n",
    "  features[\"observation.pixels.gripper\"] = Image()\n",
    "  features[\"observation.state.qpos\"] = Sequence(length=6, feature=Value(dtype=\"float32\", id=None))\n",
    "  features[\"observation.state.qvel\"] = Sequence(length=6, feature=Value(dtype=\"float32\", id=None))\n",
    "  features[\"observation.state.gripper\"] = Value(dtype=\"int8\", id=None)\n",
    "  features[\"reward\"] = Value(dtype=\"int8\", id=None)\n",
    "  features[\"action.qpos\"] = Sequence(length=6, feature=Value(dtype=\"float32\", id=None))\n",
    "  features[\"action.qvel\"] = Sequence(length=6, feature=Value(dtype=\"float32\", id=None))\n",
    "  features[\"action.gripper\"] = Value(dtype=\"int8\", id=None)\n",
    "  features[\"episode_index\"] = Value(dtype=\"int64\", id=None) # Which episode\n",
    "  features[\"frame_index\"] = Value(dtype=\"int64\", id=None) # Which frame within episode\n",
    "  features[\"timestamp\"] = Value(dtype=\"float32\", id=None)\n",
    "  # features[\"next.done\"] = Value(dtype=\"bool\", id=None)\n",
    "  features[\"index\"] = Value(dtype=\"int64\", id=None) # Which frame in the whole datasets\n",
    "\n",
    "  if not os.path.isdir(dataset_dir):\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "  successful_trajectories = 0\n",
    "  data_dict = {\"action.qpos\": [], \"action.qvel\": [], \"action.gripper\": [], \"observation.state.qpos\": [], \"observation.state.qvel\": [], \"observation.state.gripper\": [], \"observation.pixels.side\": [], \"observation.pixels.gripper\": [], \"reward\": [], \"timestamp\": [], \"episode_index\": [], \"frame_index\": [], \"index\": [],}\n",
    "\n",
    "  while successful_trajectories < num_ep:\n",
    "    ep_idx = successful_trajectories\n",
    "    print(f\"Episode {ep_idx}\")\n",
    "    observation, info = env.reset(qpos=None, box_pos=None, box_quat=None)\n",
    "    policy.reset()\n",
    "\n",
    "    ep_dict = {\"action.qpos\": [], \"action.qvel\": [], \"action.gripper\": [], \"observation.state.qpos\": [], \"observation.state.qvel\": [], \"observation.state.gripper\": [], \"observation.pixels.side\": [], \"observation.pixels.gripper\": [], \"reward\": [], \"timestamp\": [], \"frame_index\": [],}\n",
    "    ep_dict[\"episode_index\"] = [ep_idx] * num_frames\n",
    "\n",
    "    for step in range(num_frames):\n",
    "      action = policy(env.unwrapped.model, env.unwrapped.data, observation, info)\n",
    "      dq = action[\"qpos\"] - observation[\"state\"][\"qpos\"]\n",
    "      action[\"qvel\"] = Kp * dq * env.metadata[\"render_fps\"]\n",
    "\n",
    "      observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "      ep_dict[\"action.qpos\"].append(action[\"qpos\"])\n",
    "      ep_dict[\"action.qvel\"].append(action[\"qvel\"])\n",
    "      ep_dict[\"action.gripper\"].append(action[\"gripper\"])\n",
    "      ep_dict[\"observation.state.qpos\"].append(observation[\"state\"][\"qpos\"])\n",
    "      ep_dict[\"observation.state.qvel\"].append(observation[\"state\"][\"qvel\"])\n",
    "      ep_dict[\"observation.state.gripper\"].append(observation[\"state\"][\"gripper\"])\n",
    "      ep_dict[\"observation.pixels.side\"].append(observation[\"pixels\"][\"side\"])\n",
    "      ep_dict[\"observation.pixels.gripper\"].append(observation[\"pixels\"][\"gripper\"])\n",
    "      ep_dict[\"reward\"].append(reward)\n",
    "      ep_dict[\"timestamp\"].append(env.unwrapped.data.time)\n",
    "      ep_dict[\"frame_index\"].append(step)\n",
    "      # data_dict[\"index\"] = step\n",
    "    \n",
    "    if policy.done and terminated:\n",
    "      \n",
    "      # for key in ep_dict:\n",
    "      #   # Setting the dtype/transforming to a tensor makes no difference as HF Datasets stores everything in Arrow format anyway\n",
    "      #   dtype = getattr(torch, features[key].feature.dtype) if 'feature' in features[key].__dict__ and features[key].feature.dtype in torch.__dict__ else None\n",
    "      #   ep_dict[key] = torch.tensor(ep_dict[key], dtype=dtype)\n",
    "            \n",
    "      media.show_video(ep_dict[\"observation.pixels.side\"])\n",
    "      for key in ep_dict:\n",
    "        data_dict[key].extend(ep_dict[key])\n",
    "\n",
    "      base_idx = successful_trajectories*num_frames\n",
    "      data_dict[\"index\"].extend(range(base_idx, base_idx + num_frames))\n",
    "\n",
    "      successful_trajectories += 1\n",
    "    else:\n",
    "      print(\"Failed, retrying\", policy.done, terminated)\n",
    "\n",
    "  print(\"Creating dataset\")\n",
    "  # data_dict = concatenate_episodes(ep_dicts)\n",
    "  # print(data_dict, features)\n",
    "  hf_dataset = Dataset.from_dict(data_dict, features=Features(features))\n",
    "  \n",
    "  hf_dataset.save_to_disk(dataset_dir + f\"/grasp_qvel_random_{num_ep}_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.hf\")\n",
    "\n",
    "  return hf_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = gym_lite6.scripted_policy.GraspPolicy(env, 'end_effector', 'box', 'gripper_left_finger', 'gripper_right_finger', max_vel=0.2)\n",
    "# policy = gym_lite6.scripted_policy.GraspAndLiftPolicy(env, 'end_effector', 'box', 'gripper_left_finger', 'gripper_right_finger', max_vel=0.2)\n",
    "# record_episodes(env, policy, \"dataset/pickup_side_cam\", n=50)\n",
    "hf_dataset = record_episodes_to_hf(env, policy, \"../datasets/grasp\", num_ep=50, num_frames=200)\n",
    "hf_dataset.set_transform(hf_transform_to_torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lerobot_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
