{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MUJOCO_GL=egl # Had to export this before starting jupyter server\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import mediapy as media\n",
    "import torch\n",
    "import torchvision\n",
    "# torch.multiprocessing.set_start_method('spawn')\n",
    "import gym_lite6.env, gym_lite6.pickup_task\n",
    "# %env MUJOCO_GL=egl # Had to export this before starting jupyter server\n",
    "# import mujoco\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLPPolicy(torch.nn.Module):\n",
    "  def __init__(self, hidden_layer_dims, state_dims=9):\n",
    "    \"\"\"\n",
    "    state_dims: 6 for arm, 3 for gripper\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    # self.img_feature_extractor = torchvision.models.detection.backbone_utils.resnet_fpn_backbone('resnet18', )\n",
    "    \n",
    "    self.img_feature_extractor = self._create_img_feature_extractor()\n",
    "    # Resnet output is 1x512, 2 bits for gripper\n",
    "    self.actor = self._create_actor(512 + state_dims, hidden_layer_dims, state_dims)\n",
    "\n",
    "    self.sigmoid = torch.nn.Sigmoid()\n",
    "  \n",
    "  def _create_actor(self, input_size, hidden_layer_dims, output_size):\n",
    "    actor = []\n",
    "    actor.append(torch.nn.Linear(input_size, hidden_layer_dims[0]))\n",
    "    actor.append(torch.nn.ReLU())\n",
    "    for i in range(len(hidden_layer_dims) - 1):\n",
    "      actor.append(torch.nn.Linear(hidden_layer_dims[i], hidden_layer_dims[i+1]))\n",
    "      actor.append(torch.nn.ReLU())\n",
    "    actor.append(torch.nn.Linear(hidden_layer_dims[-1], output_size))\n",
    "    return torch.nn.Sequential(*actor)\n",
    "\n",
    "  def _create_img_feature_extractor(self, frozen=False):\n",
    "    \"\"\"\n",
    "    ResNet18 backbone with last fc layer chopped off\n",
    "    Weights frozen\n",
    "    Ouput shape [1, 512, 1, 1]\n",
    "    \"\"\"\n",
    "    resnet = torchvision.models.resnet18(weights='DEFAULT')\n",
    "    modules = list(resnet.children())[:-1]\n",
    "    backbone = torch.nn.Sequential(*modules)\n",
    "    backbone.requires_grad_(not frozen)\n",
    "    return backbone\n",
    "\n",
    "  def forward(self, state, image):\n",
    "    img_features = torch.squeeze(self.img_feature_extractor(image), dim=[2, 3])\n",
    "    input = torch.hstack((state, img_features))\n",
    "    out = self.actor(input)\n",
    "    # Gripper sigmoid\n",
    "    out[:, 6:8] = self.sigmoid(out[:, 6:8])\n",
    "    return out\n",
    "\n",
    "  \n",
    "  def predict(self, state, image, episode_start=None, deterministic=None):\n",
    "    return self.forward(state, image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\")\n",
    "\n",
    "policy = MLPPolicy([128, 128]).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "  def __init__(self, params) -> None:\n",
    "    # self.env = env # This breaks caching of preprocess_data\n",
    "\n",
    "    self.params = params\n",
    "\n",
    "  def normalize_qpos(self, qpos):\n",
    "    return (qpos - self.params[\"normalize_qpos\"][\"bounds_centre\"]) / self.params[\"normalize_qpos\"][\"bounds_range\"] + 0.5\n",
    "\n",
    "  def unnormalize_qpos(self, qpos):\n",
    "    return (qpos - 0.5) * self.params[\"normalize_qpos\"][\"bounds_range\"] + self.params[\"normalize_qpos\"][\"bounds_centre\"]\n",
    "  \n",
    "  def embed_gripper(self, gripper):\n",
    "    \"\"\"\n",
    "    Convert from (-1, 1) to one hot encoded\n",
    "    One hot needs them as 1d\n",
    "    \"\"\"\n",
    "    return torch.nn.functional.one_hot(gripper.flatten() + 1, num_classes=3)\n",
    "\n",
    "  def decode_gripper(self, gripper):\n",
    "    \"\"\"\n",
    "    Convert from one hot encoded to column vector in range (-1, 1)\n",
    "    \"\"\"\n",
    "    return (torch.argmax(gripper, dim=1) - 1).unsqueeze(1).to(int)\n",
    "\n",
    "  def preprocess_data(self, batch):\n",
    "    \"\"\"\n",
    "    Take a batch of data and put it in a suitable tensor format for the model\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    \n",
    "    observation_qpos = torch.tensor(batch[\"observation.state.qpos\"], dtype=torch.float32)\n",
    "    action_qpos = torch.tensor(batch[\"action.qpos\"], dtype=torch.float32)\n",
    "\n",
    "    observation_gripper = self.embed_gripper(torch.tensor(batch[\"observation.state.gripper\"], dtype=int)).to(torch.float32)\n",
    "    action_gripper = self.embed_gripper(torch.tensor(batch[\"action.gripper\"], dtype=int)).to(torch.float32)\n",
    "\n",
    "    if self.params[\"normalize_qpos\"] is not False:\n",
    "      observation_qpos = self.normalize_qpos(observation_qpos)\n",
    "      action_qpos = self.normalize_qpos(action_qpos)\n",
    "\n",
    "    out[\"preprocessed.observation.state\"] = torch.hstack((observation_qpos, observation_gripper))\n",
    "    out[\"preprocessed.action.state\"] = torch.hstack((action_qpos, action_gripper))\n",
    "    \n",
    "    # Convert to float32 with image from channel first in [0,255]\n",
    "    tf = torchvision.transforms.ToTensor()\n",
    "    out[\"preprocessed.observation.image\"] = torch.stack([tf(x) for x in batch[\"observation.pixels.side\"]])\n",
    "\n",
    "    return out\n",
    "  \n",
    "  def lerobot_preprocess(self, batch):\n",
    "    \"\"\"\n",
    "    Take a batch of data and put it in a suitable tensor format for the model\n",
    "    Batches here are as a list\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    idxs = range(len(batch[list(batch.keys())[0]]))\n",
    "    \n",
    "    if \"observation.state.gripper\" in batch and \"observation.state.qpos\" in batch:\n",
    "      observation_gripper = [self.embed_gripper(batch[\"observation.state.gripper\"][x]).to(torch.float32) for x in idxs ]\n",
    "      if self.params[\"normalize_qpos\"] is not False:\n",
    "        batch[\"observation.state.qpos\"] = [self.normalize_qpos(batch[\"observation.state.qpos\"])[x] for x in idxs if \"observation.state.gripper\" in batch]\n",
    "      out[\"preprocessed.observation.state\"] = [torch.hstack((batch[\"observation.state.qpos\"][x], observation_gripper[x].flatten())) for x in idxs]\n",
    "    \n",
    "    if \"action.gripper\" in batch and \"action.qpos\" in batch:\n",
    "      action_gripper = [self.embed_gripper(batch[\"action.gripper\"][x]).to(torch.float32) for x in idxs if \"action.gripper\" in batch]\n",
    "      if self.params[\"normalize_qpos\"] is not False:\n",
    "        batch[\"action.qpos\"] = [self.normalize_qpos(batch[\"action.qpos\"])[x] for x in idxs if \"action.qpos\" in batch]\n",
    "      out[\"preprocessed.action.state\"] = [torch.hstack((batch[\"action.qpos\"][x], action_gripper[x].flatten())) for x in idxs]\n",
    "\n",
    "    \n",
    "    # Convert to float32 with image from channel first in [0,255]\n",
    "    # tf = torchvision.transforms.ToTensor()\n",
    "    # out[\"preprocessed.observation.image\"] = torch.stack([tf(x) for x in batch[\"observation.pixels.side\"]])\n",
    "    batch.update(out)\n",
    "\n",
    "    return batch\n",
    "  \n",
    "  def evaluate_policy(self, env, policy, n):\n",
    "    avg_reward = 0\n",
    "    for i in range(n):\n",
    "      numpy_observation, info = env.reset()\n",
    "\n",
    "      # Prepare to collect every rewards and all the frames of the episode,\n",
    "      # from initial state to final state.\n",
    "      rewards = []\n",
    "      frames = []\n",
    "      action = {}\n",
    "\n",
    "      # Render frame of the initial state\n",
    "      frames.append(env.render())\n",
    "\n",
    "      step = 0\n",
    "      done = False\n",
    "      while not done and len(frames) < 300:\n",
    "        # Prepare observation for the policy running in Pytorch\n",
    "        # Get qpos in range (-1, 1), gripper is already in range (-1, 1)\n",
    "        qpos = torch.from_numpy(numpy_observation[\"state\"][\"qpos\"]).unsqueeze(0)\n",
    "        gripper = self.embed_gripper(torch.tensor(numpy_observation[\"state\"][\"gripper\"]))\n",
    "        if self.params[\"normalize_qpos\"] is not False:\n",
    "          qpos = self.normalize_qpos(qpos)\n",
    "        state = torch.hstack((qpos, gripper))\n",
    "        image = torch.from_numpy(numpy_observation[\"pixels\"][\"side\"])\n",
    "        \n",
    "        # Convert to float32 with image from channel first in [0,255]\n",
    "        # to channel last in [0,1]\n",
    "        state = state.to(torch.float32)\n",
    "        image = image.to(torch.float32) / 255\n",
    "        image = image.permute(2, 0, 1)\n",
    "\n",
    "        # Add extra (empty) batch dimension, required to forward the policy\n",
    "        # state = state.unsqueeze(0)\n",
    "        image = image.unsqueeze(0)\n",
    "\n",
    "        # Send data tensors from CPU to GPU\n",
    "        state = state.to(device, non_blocking=True)\n",
    "        image = image.to(device, non_blocking=True)\n",
    "\n",
    "        # Predict the next action with respect to the current observation\n",
    "        with torch.inference_mode():\n",
    "          raw_action = policy.predict(state, image).to(\"cpu\")\n",
    "        \n",
    "        action[\"qpos\"] = raw_action[:, :6]\n",
    "        if self.params[\"normalize_qpos\"] is not False:\n",
    "          action[\"qpos\"] = self.unnormalize_qpos(action[\"qpos\"])\n",
    "        \n",
    "        action[\"qpos\"] = action[\"qpos\"].flatten().numpy()\n",
    "        action[\"gripper\"] = self.decode_gripper(raw_action[:, 6:8]).item()\n",
    "        \n",
    "        # print(action)\n",
    "        # numpy_action = np.hstack((action[\"qpos\"], action[\"gripper\"]))\n",
    "\n",
    "        # Step through the environment and receive a new observation\n",
    "        numpy_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        # Keep track of all the rewards and frames\n",
    "        rewards.append(reward)\n",
    "        frames.append(env.render())\n",
    "\n",
    "        # The rollout is considered done when the success state is reach (i.e. terminated is True),\n",
    "        # or the maximum number of iterations is reached (i.e. truncated is True)\n",
    "        done = terminated | truncated | done\n",
    "        step += 1\n",
    "      \n",
    "      avg_reward += rewards[-1]/n\n",
    "    \n",
    "      return avg_reward, frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parser = argparse.ArgumentParser(\n",
    "#                   prog='Train Lite6 BC-MLP-MSE',\n",
    "#                   description='Train BC-MLP-MSE on Ufactory Lite6')\n",
    "# parser.add_argument('checkpoint')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# %%\n",
    "task = gym_lite6.pickup_task.PickupTask('gripper_left_finger', 'gripper_right_finger', 'box', 'floor')\n",
    "env = gym.make(\n",
    "    \"UfactoryCubePickup-v0\",\n",
    "    task=task,\n",
    "    obs_type=\"pixels_state\",\n",
    "    max_episode_steps=350,\n",
    "    visualization_width=320,\n",
    "    visualization_height=240,\n",
    ")\n",
    "observation, info = env.reset()\n",
    "# media.show_image(env.render(), width=400, height=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.common.datasets.utils import (\n",
    "    hf_transform_to_torch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {}\n",
    "\n",
    "jnt_range_low = env.unwrapped.model.jnt_range[:6, 0]\n",
    "jnt_range_high = env.unwrapped.model.jnt_range[:6, 1]\n",
    "bounds_centre = torch.tensor((jnt_range_low + jnt_range_high) / 2, dtype=torch.float32)\n",
    "bounds_range = torch.tensor(jnt_range_high - jnt_range_low, dtype=torch.float32)\n",
    "# params[\"normalize_qpos\"] = {\"bounds_centre\": bounds_centre, \"bounds_range\": bounds_range}\n",
    "params[\"normalize_qpos\"] = False\n",
    "\n",
    "trainer = Trainer(params)\n",
    "\n",
    "dataset = load_from_disk(\"BC-MLP-MSE/datasets/pickup/scripted_trajectories_50_2024-08-02_12-49-56.hf\")\n",
    "if \"from\" not in dataset.column_names:\n",
    "  first_frames=dataset.filter(lambda example: example['frame_index'] == 0)\n",
    "  from_idxs = torch.tensor(first_frames['index'])\n",
    "  to_idxs = torch.tensor(first_frames['index'][1:] + [len(dataset)])\n",
    "  episode_data_index={\"from\": from_idxs, \"to\": to_idxs}\n",
    "    \n",
    "# dataset.set_transform(hf_transform_to_torch)\n",
    "dataset.set_transform(lambda x: trainer.lerobot_preprocess(hf_transform_to_torch(x)))\n",
    "# dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action.qpos': tensor([-2.3405,  0.1678,  1.0883, -1.8791,  0.1592, -0.4317]),\n",
       " 'action.gripper': tensor(0),\n",
       " 'observation.state.qpos': tensor([-2.3405,  0.1681,  1.0880, -1.8791,  0.1592, -0.4317]),\n",
       " 'observation.state.qvel': tensor([-3.7407e-06,  8.3225e-03, -6.5398e-03,  1.8246e-04, -1.5866e-04,\n",
       "          1.7253e-06]),\n",
       " 'observation.state.gripper': tensor(0),\n",
       " 'observation.pixels.side': tensor([[[0.1608, 0.1608, 0.1608,  ..., 0.1608, 0.1608, 0.1608],\n",
       "          [0.1608, 0.1608, 0.1608,  ..., 0.1608, 0.1608, 0.1608],\n",
       "          [0.1608, 0.1608, 0.1608,  ..., 0.1608, 0.1608, 0.1608],\n",
       "          ...,\n",
       "          [0.2353, 0.2353, 0.2353,  ..., 0.1137, 0.1137, 0.1137],\n",
       "          [0.2353, 0.2353, 0.2353,  ..., 0.1137, 0.1137, 0.1137],\n",
       "          [0.2353, 0.2353, 0.2353,  ..., 0.1137, 0.1137, 0.1137]],\n",
       " \n",
       "         [[0.2706, 0.2706, 0.2706,  ..., 0.2706, 0.2706, 0.2706],\n",
       "          [0.2706, 0.2706, 0.2706,  ..., 0.2706, 0.2706, 0.2706],\n",
       "          [0.2706, 0.2706, 0.2706,  ..., 0.2706, 0.2706, 0.2706],\n",
       "          ...,\n",
       "          [0.3490, 0.3529, 0.3529,  ..., 0.2353, 0.2353, 0.2353],\n",
       "          [0.3529, 0.3529, 0.3529,  ..., 0.2353, 0.2353, 0.2353],\n",
       "          [0.3529, 0.3529, 0.3529,  ..., 0.2353, 0.2353, 0.2353]],\n",
       " \n",
       "         [[0.3804, 0.3804, 0.3804,  ..., 0.3804, 0.3804, 0.3804],\n",
       "          [0.3804, 0.3804, 0.3804,  ..., 0.3804, 0.3804, 0.3804],\n",
       "          [0.3765, 0.3765, 0.3765,  ..., 0.3765, 0.3765, 0.3765],\n",
       "          ...,\n",
       "          [0.4667, 0.4667, 0.4667,  ..., 0.3569, 0.3569, 0.3569],\n",
       "          [0.4667, 0.4667, 0.4667,  ..., 0.3569, 0.3569, 0.3569],\n",
       "          [0.4667, 0.4667, 0.4667,  ..., 0.3569, 0.3569, 0.3569]]]),\n",
       " 'observation.pixels.gripper': tensor([[[0.2039, 0.2039, 0.2039,  ..., 0.1725, 0.1529, 0.1647],\n",
       "          [0.2039, 0.2039, 0.2039,  ..., 0.1529, 0.1725, 0.1529],\n",
       "          [0.2039, 0.2039, 0.2039,  ..., 0.1647, 0.1647, 0.1608],\n",
       "          ...,\n",
       "          [0.1490, 0.1490, 0.1490,  ..., 0.2118, 0.2118, 0.2118],\n",
       "          [0.1490, 0.1490, 0.1490,  ..., 0.2118, 0.2157, 0.2157],\n",
       "          [0.1490, 0.1490, 0.1490,  ..., 0.2157, 0.2157, 0.2157]],\n",
       " \n",
       "         [[0.3020, 0.3020, 0.3020,  ..., 0.2745, 0.2549, 0.2667],\n",
       "          [0.3020, 0.3020, 0.3020,  ..., 0.2588, 0.2745, 0.2549],\n",
       "          [0.3020, 0.3020, 0.3020,  ..., 0.2667, 0.2667, 0.2627],\n",
       "          ...,\n",
       "          [0.2510, 0.2510, 0.2510,  ..., 0.3569, 0.3569, 0.3569],\n",
       "          [0.2510, 0.2510, 0.2510,  ..., 0.3569, 0.3569, 0.3569],\n",
       "          [0.2510, 0.2510, 0.2510,  ..., 0.3569, 0.3569, 0.3569]],\n",
       " \n",
       "         [[0.4039, 0.4039, 0.4039,  ..., 0.3765, 0.3529, 0.3647],\n",
       "          [0.4039, 0.4039, 0.4039,  ..., 0.3569, 0.3725, 0.3569],\n",
       "          [0.4039, 0.4039, 0.4039,  ..., 0.3647, 0.3647, 0.3647],\n",
       "          ...,\n",
       "          [0.3490, 0.3490, 0.3490,  ..., 0.5020, 0.5020, 0.5020],\n",
       "          [0.3490, 0.3490, 0.3529,  ..., 0.5020, 0.5020, 0.5020],\n",
       "          [0.3490, 0.3529, 0.3529,  ..., 0.5020, 0.5020, 0.5020]]]),\n",
       " 'reward': tensor(0),\n",
       " 'timestamp': tensor(0.0320),\n",
       " 'episode_index': tensor(0),\n",
       " 'frame_index': tensor(0),\n",
       " 'index': tensor(0),\n",
       " 'preprocessed.observation.state': tensor([-2.3405,  0.1681,  1.0880, -1.8791,  0.1592, -0.4317,  0.0000,  1.0000,\n",
       "          0.0000]),\n",
       " 'preprocessed.action.state': tensor([-2.3405,  0.1678,  1.0883, -1.8791,  0.1592, -0.4317,  0.0000,  1.0000,\n",
       "          0.0000])}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'action.gripper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[189], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/media/ssd/eugene/robotic_manipulation/lerobot_venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:2866\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2865\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2866\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ssd/eugene/robotic_manipulation/lerobot_venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:2851\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2849\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2850\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m-> 2851\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/media/ssd/eugene/robotic_manipulation/lerobot_venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:633\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    631\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m/media/ssd/eugene/robotic_manipulation/lerobot_venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:397\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable, query_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[0;32m/media/ssd/eugene/robotic_manipulation/lerobot_venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:486\u001b[0m, in \u001b[0;36mCustomFormatter.format_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 486\u001b[0m     formatted_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _unnest(formatted_batch)\n",
      "File \u001b[0;32m/media/ssd/eugene/robotic_manipulation/lerobot_venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:516\u001b[0m, in \u001b[0;36mCustomFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    514\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[1;32m    515\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[0;32m--> 516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[187], line 20\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     17\u001b[0m   episode_data_index\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m: from_idxs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m: to_idxs}\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# dataset.set_transform(hf_transform_to_torch)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m dataset\u001b[38;5;241m.\u001b[39mset_transform(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerobot_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_transform_to_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=2)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[186], line 60\u001b[0m, in \u001b[0;36mTrainer.lerobot_preprocess\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     58\u001b[0m idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mlist\u001b[39m(batch\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]]))\n\u001b[1;32m     59\u001b[0m observation_gripper \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_gripper(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation.state.gripper\u001b[39m\u001b[38;5;124m\"\u001b[39m][x])\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m idxs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation.state.gripper\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m---> 60\u001b[0m action_gripper \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_gripper(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction.gripper\u001b[39m\u001b[38;5;124m\"\u001b[39m][x])\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m idxs]\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalize_qpos\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m   observation_qpos \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_qpos(observation_qpos)[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m idxs]\n",
      "Cell \u001b[0;32mIn[186], line 60\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     58\u001b[0m idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mlist\u001b[39m(batch\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]]))\n\u001b[1;32m     59\u001b[0m observation_gripper \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_gripper(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation.state.gripper\u001b[39m\u001b[38;5;124m\"\u001b[39m][x])\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m idxs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation.state.gripper\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m---> 60\u001b[0m action_gripper \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_gripper(\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction.gripper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[x])\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m idxs]\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalize_qpos\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m   observation_qpos \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_qpos(observation_qpos)[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m idxs]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'action.gripper'"
     ]
    }
   ],
   "source": [
    "dataset.select_columns(\"timestamp\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset, CODEBASE_VERSION\n",
    "lerobot_dataset = LeRobotDataset.from_preloaded(root=Path(\"BC-MLP-MSE/datasets/scripted_trajectories_50_2024-08-02_12-49-56.hf\"),\n",
    "        split=\"train\",\n",
    "        # transform=trainer.lerobot_preprocess,\n",
    "        delta_timestamps={\"action.qpos\": [0, 0.1]},\n",
    "        # additional preloaded attributes\n",
    "        hf_dataset=dataset,\n",
    "        episode_data_index=episode_data_index,\n",
    "        # episode_data_index=dataset[\"timestamp\"],\n",
    "        info = {\n",
    "        \"codebase_version\": CODEBASE_VERSION,\n",
    "        \"fps\": env.metadata[\"render_fps\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'observation.state.gripper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[185], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlerobot_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/media/ssd/eugene/robotic_manipulation/lerobot/lerobot/common/datasets/lerobot_dataset.py:139\u001b[0m, in \u001b[0;36mLeRobotDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    136\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_dataset[idx]\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta_timestamps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[43mload_previous_and_future_frames\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode_data_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolerance_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo:\n\u001b[1;32m    148\u001b[0m     item \u001b[38;5;241m=\u001b[39m load_from_videos(\n\u001b[1;32m    149\u001b[0m         item,\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_frame_keys,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_backend,\n\u001b[1;32m    154\u001b[0m     )\n",
      "File \u001b[0;32m/media/ssd/eugene/robotic_manipulation/lerobot/lerobot/common/datasets/utils.py:255\u001b[0m, in \u001b[0;36mload_previous_and_future_frames\u001b[0;34m(item, hf_dataset, episode_data_index, delta_timestamps, tolerance_s)\u001b[0m\n\u001b[1;32m    252\u001b[0m ep_data_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(ep_data_id_from, ep_data_id_to, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# load timestamps\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m ep_timestamps \u001b[38;5;241m=\u001b[39m \u001b[43mhf_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mep_data_id_from\u001b[49m\u001b[43m:\u001b[49m\u001b[43mep_data_id_to\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    256\u001b[0m ep_timestamps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(ep_timestamps)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# we make the assumption that the timestamps are sorted\u001b[39;00m\n",
      "File \u001b[0;32m/media/ssd/eugene/robotic_manipulation/lerobot_venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:2866\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2865\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2866\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ssd/eugene/robotic_manipulation/lerobot_venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:2851\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2849\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2850\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m-> 2851\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/media/ssd/eugene/robotic_manipulation/lerobot_venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:633\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    631\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m/media/ssd/eugene/robotic_manipulation/lerobot_venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:401\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ssd/eugene/robotic_manipulation/lerobot_venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:516\u001b[0m, in \u001b[0;36mCustomFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    514\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[1;32m    515\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[0;32m--> 516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[181], line 20\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     17\u001b[0m   episode_data_index\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m: from_idxs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m: to_idxs}\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# dataset.set_transform(hf_transform_to_torch)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m dataset\u001b[38;5;241m.\u001b[39mset_transform(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerobot_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_transform_to_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=2)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[180], line 60\u001b[0m, in \u001b[0;36mTrainer.lerobot_preprocess\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     58\u001b[0m idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mlist\u001b[39m(batch\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]]))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# if \"observation.state.gripper\" in batch:\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m observation_gripper \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_gripper(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation.state.gripper\u001b[39m\u001b[38;5;124m\"\u001b[39m][x])\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m idxs]\n\u001b[1;32m     61\u001b[0m action_gripper \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_gripper(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction.gripper\u001b[39m\u001b[38;5;124m\"\u001b[39m][x])\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m idxs]\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalize_qpos\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[180], line 60\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     58\u001b[0m idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mlist\u001b[39m(batch\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]]))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# if \"observation.state.gripper\" in batch:\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m observation_gripper \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_gripper(\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservation.state.gripper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[x])\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m idxs]\n\u001b[1;32m     61\u001b[0m action_gripper \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_gripper(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction.gripper\u001b[39m\u001b[38;5;124m\"\u001b[39m][x])\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m idxs]\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalize_qpos\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'observation.state.gripper'"
     ]
    }
   ],
   "source": [
    "lerobot_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "curr_time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "hidden_layer_dims = '_'.join([str(x.out_features) for x in policy.actor[:-1] if 'out_features' in x.__dict__])\n",
    "OUTPUT_FOLDER=f'../ckpts/lite6_pick_place_h{hidden_layer_dims}_{curr_time}'\n",
    "Path(OUTPUT_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"../runs/lite6_pick_place/{curr_time}\")\n",
    "\n",
    "n_epoch = 20\n",
    "step = 0\n",
    "for epoch in range(n_epoch):\n",
    "  policy.train()\n",
    "  end = time.time()\n",
    "  for batch in tqdm(dataloader):\n",
    "    data_load_time = time.time()\n",
    "\n",
    "    # Send data tensors from CPU to GPU\n",
    "    state = batch[\"preprocessed.observation.state\"].to(device, non_blocking=True)\n",
    "    image = batch[\"preprocessed.observation.image\"].to(device, non_blocking=True)\n",
    "    a_hat = batch[\"preprocessed.action.state\"].to(device, non_blocking=True)\n",
    "\n",
    "    gpu_load_time = time.time()\n",
    "\n",
    "    a_pred = policy.predict(state, image)\n",
    "\n",
    "    pred_time = time.time()\n",
    "\n",
    "    loss = loss_fn(a_pred, a_hat)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_time = time.time()\n",
    "\n",
    "    writer.add_scalar(\"Loss/train\", loss.item(), step)\n",
    "    writer.add_scalar(\"Time/data_load\", data_load_time - end, step)\n",
    "    writer.add_scalar(\"Time/gpu_transfer\", gpu_load_time - data_load_time, step)\n",
    "    writer.add_scalar(\"Time/pred_time\", pred_time - gpu_load_time, step)\n",
    "    writer.add_scalar(\"Time/train_time\", train_time - pred_time, step)\n",
    "    writer.add_scalar(\"Time/step_time\", time.time() - end, step)\n",
    "\n",
    "    step += 1\n",
    "    end = time.time()\n",
    "  \n",
    "  if epoch % 2 == 0 or epoch == n_epoch-1:\n",
    "    # Evaluate\n",
    "    policy.eval()\n",
    "    print(f\"Epoch: {epoch+1}/{n_epoch}, steps: {step}, loss: {loss.item()}\")\n",
    "    avg_reward, frames = trainer.evaluate_policy(env, policy, 5)\n",
    "    media.write_video(OUTPUT_FOLDER + f\"/epoch_{epoch}.mp4\", frames, fps=env.metadata[\"render_fps\"])\n",
    "    print(\"avg reward: \", avg_reward)\n",
    "    writer.add_scalar(\"Reward/val\", avg_reward, step)\n",
    "    # _, frames = evaluate_policy(policy, env, 1, visualise=True)\n",
    "    writer.add_images(\"Image\", np.stack([frames[x].transpose(2, 0, 1) for x in range(0, len(frames), 50)], axis=0), step)\n",
    "  \n",
    "    writer.add_scalar(\"Time/eval_time\", time.time() - end, step)\n",
    "\n",
    "\n",
    "  if epoch % 10 == 0 or epoch == n_epoch-1:\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'params': params,\n",
    "            'policy_state_dict': policy.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, OUTPUT_FOLDER + f'/epoch_{epoch}.pt')\n",
    "  \n",
    "writer.flush()\n",
    "\n",
    "writer.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
