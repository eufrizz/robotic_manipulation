{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import mediapy as media\n",
    "import torch\n",
    "# torch.multiprocessing.set_start_method('spawn')\n",
    "import gym_lite6.env, gym_lite6.pickup_task, gym_lite6.policies.mlp\n",
    "import time\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from importlib import reload\n",
    "reload(gym_lite6.policies.mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args\n",
    "checkpoint = None\n",
    "eval = False\n",
    "n_epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "task = gym_lite6.pickup_task.GraspAndLiftTask('gripper_left_finger', 'gripper_right_finger', 'box', 'floor')\n",
    "env = gym.make(\n",
    "    \"UfactoryCubePickup-v0\",\n",
    "    task=task,\n",
    "    obs_type=\"pixels_state\",\n",
    "    max_episode_steps=350,\n",
    "    visualization_width=320,\n",
    "    visualization_height=240,\n",
    ")\n",
    "observation, info = env.reset()\n",
    "# media.show_image(env.render(), width=400, height=400)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "from lerobot.common.datasets.utils import hf_transform_to_torch\n",
    "dataset_path = \"datasets/50_single_2024-09-16_15-07-50.hf\"\n",
    "dataset = load_from_disk(dataset_path)\n",
    "if \"from\" not in dataset.column_names:\n",
    "  first_frames=dataset.filter(lambda example: example['frame_index'] == 0)\n",
    "  from_idxs = torch.tensor(first_frames['index'])\n",
    "  to_idxs = torch.tensor(first_frames['index'][1:] + [len(dataset)])\n",
    "  episode_data_index={\"from\": from_idxs, \"to\": to_idxs}\n",
    "    \n",
    "dataset.set_transform(hf_transform_to_torch)\n",
    "# dataset.set_transform(lambda x: interface.lerobot_preprocess(hf_transform_to_torch(x)))\n",
    "# dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset, CODEBASE_VERSION\n",
    "lerobot_dataset = LeRobotDataset.from_preloaded(root=Path(dataset_path),\n",
    "        split=\"train\",\n",
    "        delta_timestamps={\"action.qpos\": [0, 0.1], \"action.gripper\": [0, 0.1]},\n",
    "        # additional preloaded attributes\n",
    "        hf_dataset=dataset,\n",
    "        episode_data_index=episode_data_index,\n",
    "        info = {\n",
    "          \"codebase_version\": CODEBASE_VERSION,\n",
    "          \"fps\": env.metadata[\"render_fps\"]\n",
    "        })\n",
    "\n",
    "\n",
    "# %%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {}\n",
    "if checkpoint is not None:\n",
    "  checkpoint = torch.load(checkpoint)\n",
    "  start_epoch = checkpoint[\"epoch\"] + 1\n",
    "  step = checkpoint[\"step\"]\n",
    "  params = checkpoint[\"params\"]\n",
    "  if not \"hidden_layer_dims\" in params:\n",
    "    params[\"hidden_layer_dims\"] = [64, 64, 64]\n",
    "  if not \"dropout\" in params:\n",
    "    params[\"dropout\"] = False\n",
    "else:\n",
    "  print(\"train from scratch\")\n",
    "  start_epoch = 0\n",
    "  step = 0\n",
    "  params = {}\n",
    "  params[\"normalize_qpos\"] = True\n",
    "  params[\"dropout\"] = False\n",
    "  params[\"hidden_layer_dims\"] = [64, 64, 64]\n",
    "  params[\"use_obs_vel\"] = True\n",
    "\n",
    "# Override these params\n",
    "params[\"lr\"] = 1e-3\n",
    "params[\"device\"] = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\")\n",
    "\n",
    "input_state_dims = 15 if params[\"use_obs_vel\"] else 9\n",
    "policy = gym_lite6.policies.mlp.MLPPolicy(params[\"hidden_layer_dims\"], input_state_dims=input_state_dims, dropout=params[\"dropout\"]).to(params[\"device\"])\n",
    "loss=torch.tensor(0)\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "if checkpoint is not None:\n",
    "  policy.load_state_dict(checkpoint[\"policy_state_dict\"])\n",
    "  optimizer = torch.optim.Adam(policy.parameters(), lr=params[\"lr\"])\n",
    "  optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "  loss = checkpoint[\"loss\"]\n",
    "  print(f\"Loaded checkpoint at epoch {start_epoch}\")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "dataloader = DataLoader(\n",
    "        lerobot_dataset,\n",
    "        num_workers=4,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        # sampler=sampler,\n",
    "        pin_memory=params[\"device\"].type != \"cpu\",\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "# TODO: Maybe check that these are the same as what is loaded from checkpoint?\n",
    "jnt_range_low = env.unwrapped.model.jnt_range[:6, 0]\n",
    "jnt_range_high = env.unwrapped.model.jnt_range[:6, 1]\n",
    "bounds_centre = torch.tensor((jnt_range_low + jnt_range_high) / 2, dtype=torch.float32)\n",
    "bounds_range = torch.tensor(jnt_range_high - jnt_range_low, dtype=torch.float32)\n",
    "params[\"joint_bounds\"] = {\"centre\": bounds_centre, \"range\": bounds_range}\n",
    "\n",
    "interface = gym_lite6.policies.mlp.Interface(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "  batch = interface.batched_preprocess(batch)\n",
    "  print(batch[\"preprocessed.observation.state.qpos\"])# + batch[\"preprocessed.observation.state.qvel\"]\n",
    "  # print(batch[\"preprocessed.observation.state.qvel\"])\n",
    "  # print(batch[\"observation.state.qvel\"])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "curr_time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "hidden_layer_dims = '_'.join([str(x.out_features) for x in policy.actor[:-1] if 'out_features' in x.__dict__])\n",
    "OUTPUT_FOLDER=f'ckpts/lite6_grasp_h{hidden_layer_dims}_{curr_time}'\n",
    "Path(OUTPUT_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if eval:\n",
    "  print(\"Evaluating...\")\n",
    "  policy.eval()\n",
    "  print(f\"Epoch: {start_epoch}, steps: {step}, loss: {loss.item()}\")\n",
    "  avg_reward, frames = interface.evaluate_policy(env, policy, 5)\n",
    "  media.write_video(OUTPUT_FOLDER + f\"/epoch_{start_epoch}.mp4\", frames, fps=env.metadata[\"render_fps\"])\n",
    "\n",
    "else:\n",
    "  writer = SummaryWriter(log_dir=f\"runs/lite6_grasp/{curr_time}\")\n",
    "\n",
    "  end_epoch = start_epoch+n_epochs\n",
    "  for epoch in range(start_epoch, end_epoch+1):\n",
    "    policy.train()\n",
    "    end = time.time()\n",
    "    for batch in tqdm(dataloader):\n",
    "      data_load_time = time.time()\n",
    "\n",
    "      batch = interface.batched_preprocess(batch)\n",
    "\n",
    "      # Send data tensors from CPU to GPU\n",
    "      state = (batch[\"preprocessed.observation.state.qpos\"] + batch[\"preprocessed.observation.state.qvel\"]).to(params[\"device\"], non_blocking=True)\n",
    "      image_side = batch[\"observation.pixels.side\"].to(params[\"device\"], non_blocking=True)\n",
    "      image_gripper = batch[\"observation.pixels.gripper\"].to(params[\"device\"], non_blocking=True)\n",
    "\n",
    "      # Because we sample the action ahead in time [0, 0.1], it has an extra dimension, and we select the last dim\n",
    "      a_hat = batch[\"preprocessed.action.state.qpos\"][:, -1, :].to(params[\"device\"], non_blocking=True)\n",
    "      # print([(x, batch[x]) for x in batch if \"pixels\" not in x])\n",
    "\n",
    "      gpu_load_time = time.time()\n",
    "\n",
    "      a_pred = policy.predict(state, image_side, image_gripper)\n",
    "\n",
    "      pred_time = time.time()\n",
    "\n",
    "      loss = loss_fn(a_pred, a_hat)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_time = time.time()\n",
    "\n",
    "      writer.add_scalar(\"Loss/train\", loss.item(), step)\n",
    "      writer.add_scalar(\"Time/data_load\", data_load_time - end, step)\n",
    "      writer.add_scalar(\"Time/gpu_transfer\", gpu_load_time - data_load_time, step)\n",
    "      writer.add_scalar(\"Time/pred_time\", pred_time - gpu_load_time, step)\n",
    "      writer.add_scalar(\"Time/train_time\", train_time - pred_time, step)\n",
    "      writer.add_scalar(\"Time/step_time\", time.time() - end, step)\n",
    "\n",
    "      step += 1\n",
    "      end = time.time()\n",
    "    \n",
    "    if epoch in [1, 2, 4, 8, 16, 32, 64]:\n",
    "      # Evaluate\n",
    "      policy.eval()\n",
    "      print(f\"Epoch: {epoch}/{end_epoch}, steps: {step}, loss: {loss.item()}\")\n",
    "      qpos0 = np.array([0, 0.541, 1.49 , 2.961, 0.596, 0.203])\n",
    "      box_pos0 = np.array([0.2, 0, 0.0])\n",
    "      box_quat0 = None\n",
    "      avg_reward, frames = interface.evaluate_policy(env, policy, 5, qpos0, box_pos0, box_quat0)\n",
    "      media.write_video(OUTPUT_FOLDER + f\"/epoch_{epoch}.mp4\", frames, fps=env.metadata[\"render_fps\"])\n",
    "      print(\"avg reward: \", avg_reward)\n",
    "      writer.add_scalar(\"Reward/val\", avg_reward, step)\n",
    "      # _, frames = evaluate_policy(policy, env, 1, visualise=True)\n",
    "      writer.add_images(\"Image\", np.stack([frames[x].transpose(2, 0, 1) for x in range(0, len(frames), 50)], axis=0), step)\n",
    "    \n",
    "      writer.add_scalar(\"Time/eval_time\", time.time() - end, step)\n",
    "\n",
    "\n",
    "    # if epoch % 10 == 0 or epoch == end_epoch:\n",
    "    if epoch in [1, 2, 4, 8, 10, 16, 20, 32, 40, 64]:\n",
    "      torch.save({\n",
    "              'epoch': epoch,\n",
    "              'step': step,\n",
    "              'params': params,\n",
    "              'policy_state_dict': policy.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': loss,\n",
    "              }, OUTPUT_FOLDER + f'/epoch_{epoch}.pt')\n",
    "    \n",
    "  writer.flush()\n",
    "\n",
    "  writer.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
