{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from lerobot.common.datasets.utils import hf_transform_to_torch\n",
    "# from lerobot.common.datasets.lerobot_dataset import LeRobotDataset, CODEBASE_VERSION\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_path = \"../datasets/grasp_100_2024-09-06_17-03-47.hf\"\n",
    "dataset = load_from_disk(dataset_path)\n",
    "# if \"from\" not in dataset.column_names:\n",
    "#   first_frames=dataset.filter(lambda example: example['frame_index'] == 0)\n",
    "#   from_idxs = torch.tensor(first_frames['index'])\n",
    "#   to_idxs = torch.tensor(first_frames['index'][1:] + [len(dataset)])\n",
    "#   episode_data_index={\"from\": from_idxs, \"to\": to_idxs}\n",
    "    \n",
    "dataset.set_transform(hf_transform_to_torch)\n",
    "# dataset = dataset.with_format(\"torch\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "        dataset,\n",
    "        num_workers=4,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        # sampler=sampler,\n",
    "        # pin_memory=params[\"device\"].type != \"cpu\",\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "\n",
    "curr_time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "dataset_name = Path(dataset_path).stem\n",
    "CKPT_DIR=f'ckpts/resnet_byol_{dataset_name}_{curr_time}'\n",
    "TENSORBOARD_DIR=f'runs/resnet_byol_{dataset_name}_{curr_time}'\n",
    "Path(CKPT_DIR).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from byol_pytorch import BYOL\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\")\n",
    "\n",
    "# Remove the fc/classification layer\n",
    "resnet = torchvision.models.resnet18(weights='DEFAULT')\n",
    "modules = list(resnet.children())[:-1]\n",
    "backbone = torch.nn.Sequential(*modules)\n",
    "net = backbone\n",
    "\n",
    "\n",
    "learner = BYOL(\n",
    "    net,\n",
    "    image_size = 240,\n",
    "    hidden_layer = 8\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = torch.optim.Adam(learner.parameters(), lr=3e-4)\n",
    "\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_DIR)\n",
    "epoch = 0\n",
    "step = 0\n",
    "for epoch in range(10):\n",
    "  epoch +=1\n",
    "  end = time.time()\n",
    "  for batch in tqdm(dataloader):\n",
    "      data_load_time = time.time()\n",
    "      step +=1\n",
    "\n",
    "      images = torch.cat((batch['observation.pixels.side'], batch['observation.pixels.gripper']), dim=0).to(device)\n",
    "      gpu_load_time = time.time()\n",
    "\n",
    "      loss = learner(images)\n",
    "      pred_time = time.time()\n",
    "\n",
    "      opt.zero_grad()\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "      learner.update_moving_average() # update moving average of target encoder\n",
    "      train_time = time.time()\n",
    "\n",
    "      writer.add_scalar(\"Loss/train\", loss.item(), step)\n",
    "      writer.add_scalar(\"Time/data_load\", data_load_time - end, step)\n",
    "      writer.add_scalar(\"Time/gpu_transfer\", gpu_load_time - data_load_time, step)\n",
    "      writer.add_scalar(\"Time/pred_time\", pred_time - gpu_load_time, step)\n",
    "      writer.add_scalar(\"Time/train_time\", train_time - pred_time, step)\n",
    "      writer.add_scalar(\"Time/step_time\", time.time() - end, step)\n",
    "\n",
    "# save your improved network\n",
    "torch.save({'policy_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'loss': loss,\n",
    "            'epoch': epoch,\n",
    "            'step': step,\n",
    "            }, CKPT_DIR + f'/epoch_{epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules = list(net.children())[:-1]\n",
    "# backbone = torch.nn.Sequential(*modules).to(device)\n",
    "dataloader = DataLoader(\n",
    "        dataset,\n",
    "        num_workers=4,\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        # sampler=sampler,\n",
    "        # pin_memory=params[\"device\"].type != \"cpu\",\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "checkpoint = torch.load(\"../VINN/ckpts/resnet_byol_grasp_100_2024-09-06_17-03-47_2024-10-12_21-08/epoch_10.pt\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\")\n",
    "\n",
    "# Remove the fc/classification layer\n",
    "resnet = torchvision.models.resnet18()\n",
    "modules = list(resnet.children())[:-1]\n",
    "backbone = torch.nn.Sequential(*modules)\n",
    "net = backbone\n",
    "net.load_state_dict(checkpoint[\"policy_state_dict\"])\n",
    "net = net.to(device)\n",
    "net.eval()\n",
    "epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "# Save embeddings of all images\n",
    "img_cols = {\"observation.pixels.side\": \"observation.vector.side\", \"observation.pixels.gripper\" : \"observation.vector.gripper\"}\n",
    "vec_columns = {v:[] for k,v in img_cols.items()}\n",
    "for batch in tqdm(dataloader):\n",
    "    with torch.inference_mode():\n",
    "      for img_col in img_cols:\n",
    "        # Batch inference to get embeddings\n",
    "        embedding = backbone(batch[img_col].to(device)).to(\"cpu\")\n",
    "        # Add the embedding (size 512) for each image to the column\n",
    "        vec_columns[img_cols[img_col]].extend([e.numpy() for e in embedding.squeeze()])\n",
    "\n",
    "# vec_columns = {k: [r.numpy() for r in v] for k, v in vec_columns.items()}\n",
    "for col in vec_columns:\n",
    "  assert(len(vec_columns[col]) == len(dataset))\n",
    "  dataset = dataset.add_column(name=col, column=vec_columns[col])\n",
    "\n",
    "dataset.set_transform(None)\n",
    "dataset.save_to_disk(f\"../datasets/byol/{dataset_name}_epoch_{epoch}.hf\")\n",
    "dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(f\"../datasets/byol/{dataset_name}_epoch_{epoch}.hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and model if not continuing from before\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "dataset_path = \"../datasets/byol/grasp_100_2024-09-06_17-03-47_epoch_10.hf\"\n",
    "dataset_name = Path(dataset_path).stem\n",
    "dataset = load_from_disk(dataset_path)\n",
    "dataset.set_format(\"torch\")\n",
    "\n",
    "checkpoint = torch.load(\"../VINN/ckpts/resnet_byol_grasp_100_2024-09-06_17-03-47_2024-10-12_21-08/epoch_10.pt\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\")\n",
    "\n",
    "# Remove the fc/classification layer\n",
    "resnet = torchvision.models.resnet18()\n",
    "modules = list(resnet.children())[:-1]\n",
    "backbone = torch.nn.Sequential(*modules)\n",
    "net = backbone\n",
    "net.load_state_dict(checkpoint[\"policy_state_dict\"])\n",
    "net = net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_lite6.env, gym_lite6.pickup_task, gym_lite6.utils\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import mediapy as media\n",
    "\n",
    "\n",
    "task = gym_lite6.pickup_task.GraspAndLiftTask('gripper_left_finger', 'gripper_right_finger', 'box', 'floor')\n",
    "env = gym.make(\n",
    "    \"UfactoryCubePickup-v0\",\n",
    "    task=task,\n",
    "    obs_type=\"pixels_state\",\n",
    "    max_episode_steps=300,\n",
    "    visualization_width=320,\n",
    "    visualization_height=240\n",
    ")\n",
    "observation, info = env.reset()\n",
    "media.show_image(env.render())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def dist_metric(x,y):\n",
    "  \"\"\"\n",
    "  L2 norm\n",
    "  \"\"\"\n",
    "  return torch.norm(x-y, dim=1)\n",
    "\n",
    "def calculate_nearest_neighbours(query, data, k=1, verbose=False):\n",
    "  t0 = time.time()\n",
    "  dists = []\n",
    "  assert len(data[0].shape) == 1, f\"Invalid shape, should be 1D: {data.shape}\"\n",
    "  # Takes around 0.25s for 20000 samples\n",
    "  dists = dist_metric(query, data).cpu()\n",
    "  heap = [(dists[i].item(), i) for i in range(data.shape[0])]\n",
    "  \n",
    "  t1 = time.time()\n",
    "  heapq.heapify(heap)\n",
    "  t2 = time.time()\n",
    "\n",
    "  out = []\n",
    "  for i in range(k):\n",
    "    out.append(heapq.heappop(heap))\n",
    "  \n",
    "  if verbose:\n",
    "    print(f\"Times: get dists: {t1-t0}, heapify: {t2-t1}\")\n",
    "  return out\n",
    "        \n",
    "def calculate_action(dists, dataset, key='action.qpos'):\n",
    "  \"\"\"\n",
    "  Local weighting\n",
    "  \"\"\"\n",
    "  if len(dists) > 1:\n",
    "    softmin = torch.nn.Softmin(dim=0)\n",
    "    top_k_weights = softmin(torch.tensor([d[0] for d in dists])).tolist()\n",
    "    action = sum([top_k_weights[i] * dataset[dists[i][1]][key] for i in range(len(dists))])\n",
    "  else:\n",
    "    action = dataset[dists[0][1]][key]\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "k=5\n",
    "frames = []\n",
    "nn_frames = [[] for _ in range(k)]\n",
    "action = {}\n",
    "ep_dict = {\"action.qpos\": [], \"action.gripper\": [], \"observation.state.qpos\": [], \"observation.state.qvel\": [], \"observation.state.gripper\": [], \"observation.pixels.side\": [], \"observation.pixels.gripper\": [], \"reward\": [], \"timestamp\": [], \"frame_index\": [],}\n",
    "\n",
    "vector_data = dataset[\"observation.vector.side\"].to(device)\n",
    "\n",
    "step = 0\n",
    "done = False\n",
    "net.eval()\n",
    "while step < 200:\n",
    "\n",
    "  t0 = time.time()\n",
    "  image_side = (torch.from_numpy(observation[\"pixels\"][\"side\"]).permute(2, 0, 1).unsqueeze(0) / 255).to(device)\n",
    "  # image_gripper = torch.from_numpy(observation[\"pixels\"][\"gripper\"]).permute(2, 0, 1).unsqueeze(0) / 255\n",
    "\n",
    "  with torch.inference_mode():\n",
    "    embedding_side = net(image_side).squeeze()\n",
    "    # embedding_gripper = net(image_side, image_gripper).to(\"cpu\")\n",
    "  t1 = time.time()\n",
    "  \n",
    "  nn = calculate_nearest_neighbours(embedding_side, vector_data, k=k)\n",
    "\n",
    "  t2 = time.time()\n",
    "\n",
    "  # Save the nearest image in the dataset for comparison\n",
    "  stacked_frame = np.hstack([observation[\"pixels\"][\"side\"]] + [dataset[nn[i][1]][\"observation.pixels.side\"].permute(1, 2, 0).numpy() for i in range(k)])\n",
    "  frames.append(stacked_frame)\n",
    "\n",
    "  action[\"qpos\"] = calculate_action(nn, dataset, key='action.qpos').numpy()\n",
    "  action[\"gripper\"] = round(calculate_action(nn, dataset, key='action.gripper').item())\n",
    "\n",
    "  \n",
    "  ep_dict[\"observation.state.qpos\"].append(observation[\"state\"][\"qpos\"])\n",
    "  ep_dict[\"observation.state.qvel\"].append(observation[\"state\"][\"qvel\"])\n",
    "  ep_dict[\"observation.state.gripper\"].append(observation[\"state\"][\"gripper\"])\n",
    "  ep_dict[\"timestamp\"].append(env.unwrapped.data.time)\n",
    "\n",
    "  # Step through the environment and receive a new observation\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "  \n",
    "  # ep_dict[\"observation.pixels.side\"].append(observation[\"pixels\"][\"side\"])\n",
    "  # ep_dict[\"observation.pixels.gripper\"].append(observation[\"pixels\"][\"gripper\"])\n",
    "  ep_dict[\"reward\"].append(reward)\n",
    "  ep_dict[\"action.qpos\"].append(action[\"qpos\"])\n",
    "  ep_dict[\"action.gripper\"].append(action[\"gripper\"])\n",
    "  # ep_dict[\"frame_index\"].append(step)\n",
    "  # action = calculate_action(nn, key='observation.state.qvel')\n",
    "    \n",
    "  done = truncated | done | terminated\n",
    "  step += 1\n",
    "\n",
    "  print(f\"Timing: inference: {t1-t0}, nn: {t2-t1}, rest: {time.time()-t2}\")\n",
    "\n",
    "\n",
    "ep_dict[\"observation.state.qpos\"].append(observation[\"state\"][\"qpos\"])\n",
    "ep_dict[\"observation.state.qvel\"].append(observation[\"state\"][\"qvel\"])\n",
    "ep_dict[\"observation.state.gripper\"].append(observation[\"state\"][\"gripper\"])\n",
    "ep_dict[\"timestamp\"].append(env.unwrapped.data.time)\n",
    "\n",
    "# Append dummy nans to the actions so we have the same number of samples can plot it\n",
    "ep_dict[\"action.qpos\"].append(np.array([np.nan] * len(action[\"qpos\"])))\n",
    "ep_dict[\"action.gripper\"].append(np.array([np.nan]))\n",
    "# ep_dict[\"reward\"].append(np.array([np.nan]))\n",
    "\n",
    "avg_reward = sum(ep_dict[\"reward\"])/len(ep_dict[\"reward\"])\n",
    "print(f\"Avg reward: {avg_reward}\")\n",
    "media.show_video(frames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_dict[\"reward\"].append(np.array([np.nan]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_lite6.utils.plot_dict_of_arrays(ep_dict, \"timestamp\", keys=[\"action.qpos\", \"observation.state.qpos\"], sharey=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qpos search\n",
    "First search for \"similar\" qpos in the data, and then of these select most visually similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing qpos search\n",
    "observation, info = env.reset()\n",
    "print(observation[\"state\"][\"qpos\"])\n",
    "qpos_nn = calculate_nearest_neighbours(torch.from_numpy(observation[\"state\"][\"qpos\"]), dataset[\"observation.state.qpos\"], k=10)\n",
    "qpos_candidates = [n[1] for n in qpos_nn]\n",
    "# print(dataset[nn[0][1]][\"aobservation.state.qpos\"])\n",
    "\n",
    "# observation, info = env.reset(qpos=action[\"qpos\"])\n",
    "media.show_image(observation[\"pixels\"][\"side\"])\n",
    "\n",
    "frames = []\n",
    "for n in qpos_nn:\n",
    "  print(n, dataset[n[1]][\"observation.state.qpos\"])\n",
    "  observation, info = env.reset(qpos=dataset[n[1]][\"observation.state.qpos\"])\n",
    "  frames.append(observation[\"pixels\"][\"side\"])\n",
    "media.show_images(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing qpos search then vision\n",
    "image_side = (torch.from_numpy(observation[\"pixels\"][\"side\"]).permute(2, 0, 1).unsqueeze(0) / 255).to(device)\n",
    "# image_gripper = torch.from_numpy(observation[\"pixels\"][\"gripper\"]).permute(2, 0, 1).unsqueeze(0) / 255\n",
    "\n",
    "with torch.inference_mode():\n",
    "  embedding_side = net(image_side).squeeze().cpu()\n",
    "\n",
    "vis_nn = calculate_nearest_neighbours(embedding_side, dataset[qpos_candidates][\"observation.vector.side\"], k=1)\n",
    "\n",
    "nn = [(n[0],qpos_candidates[n[1]]) for n in vis_nn]\n",
    "print(qpos_candidates)\n",
    "print(nn)\n",
    "\n",
    "for n in nn:\n",
    "  print(n[1])\n",
    "  observation, info = env.reset(qpos=dataset[n[1]][\"observation.state.qpos\"])\n",
    "  media.show_image(observation[\"pixels\"][\"side\"])\n",
    "\n",
    "print(\"action: \")\n",
    "action[\"qpos\"] = calculate_action(nn, dataset, key='action.qpos').numpy()\n",
    "\n",
    "observation, info = env.reset(qpos=dataset[n[1]][\"observation.state.qpos\"])\n",
    "media.show_image(observation[\"pixels\"][\"side\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in nn:\n",
    "  print(n[1])\n",
    "  observation, info = env.reset(qpos=dataset[n[1]][\"observation.state.qpos\"])\n",
    "  media.show_image(observation[\"pixels\"][\"side\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "k=3\n",
    "frames = []\n",
    "nn_frames = [[] for _ in range(k)]\n",
    "action = {}\n",
    "ep_dict = {\"action.qpos\": [], \"action.gripper\": [], \"observation.state.qpos\": [], \"observation.state.qvel\": [], \"observation.state.gripper\": [], \"observation.pixels.side\": [], \"observation.pixels.gripper\": [], \"reward\": [], \"timestamp\": [], \"frame_index\": [],}\n",
    "\n",
    "vector_data = dataset[\"observation.vector.side\"]\n",
    "\n",
    "step = 0\n",
    "done = False\n",
    "net.eval()\n",
    "while step < 50:\n",
    "\n",
    "  t0 = time.time()\n",
    "  image_side = (torch.from_numpy(observation[\"pixels\"][\"side\"]).permute(2, 0, 1).unsqueeze(0) / 255).to(device)\n",
    "  # image_gripper = torch.from_numpy(observation[\"pixels\"][\"gripper\"]).permute(2, 0, 1).unsqueeze(0) / 255\n",
    "\n",
    "  with torch.inference_mode():\n",
    "    embedding_side = net(image_side).squeeze().cpu()\n",
    "    # embedding_gripper = net(image_side, image_gripper).to(\"cpu\")\n",
    "  t1 = time.time()\n",
    "\n",
    "  # Find nearest neighbours to qpos\n",
    "  qpos_nn = calculate_nearest_neighbours(torch.from_numpy(observation[\"state\"][\"qpos\"]), dataset[\"observation.state.qpos\"], k=30)\n",
    "  qpos_candidates = [n[1] for n in qpos_nn]\n",
    "  \n",
    "  # Find most visually similar\n",
    "  vis_nn = calculate_nearest_neighbours(embedding_side, dataset[qpos_candidates][\"observation.vector.side\"], k=k)\n",
    "  \n",
    "  nn = [(n[0],qpos_candidates[n[1]]) for n in vis_nn]\n",
    "  print(nn)\n",
    "  t2 = time.time()\n",
    "\n",
    "  # Save the nearest image in the dataset for comparison\n",
    "  stacked_frame = np.hstack([observation[\"pixels\"][\"side\"]] + [dataset[nn[i][1]][\"observation.pixels.side\"].permute(1, 2, 0).numpy() for i in range(k)])\n",
    "  frames.append(stacked_frame)\n",
    "\n",
    "  action[\"qpos\"] = calculate_action(nn, dataset, key='action.qpos').numpy()\n",
    "  action[\"gripper\"] = round(calculate_action(nn, dataset, key='action.gripper').item())\n",
    "  print(action)\n",
    "\n",
    "  \n",
    "  ep_dict[\"observation.state.qpos\"].append(observation[\"state\"][\"qpos\"])\n",
    "  ep_dict[\"observation.state.qvel\"].append(observation[\"state\"][\"qvel\"])\n",
    "  ep_dict[\"observation.state.gripper\"].append(observation[\"state\"][\"gripper\"])\n",
    "  ep_dict[\"timestamp\"].append(env.unwrapped.data.time)\n",
    "\n",
    "  # Step through the environment and receive a new observation\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "  \n",
    "  # ep_dict[\"observation.pixels.side\"].append(observation[\"pixels\"][\"side\"])\n",
    "  # ep_dict[\"observation.pixels.gripper\"].append(observation[\"pixels\"][\"gripper\"])\n",
    "  ep_dict[\"reward\"].append(reward)\n",
    "  ep_dict[\"action.qpos\"].append(action[\"qpos\"])\n",
    "  ep_dict[\"action.gripper\"].append(action[\"gripper\"])\n",
    "  # ep_dict[\"frame_index\"].append(step)\n",
    "  # action = calculate_action(nn, key='observation.state.qvel')\n",
    "    \n",
    "  done = truncated | done | terminated\n",
    "  step += 1\n",
    "\n",
    "  print(f\"Timing: inference: {t1-t0}, nn: {t2-t1}, rest: {time.time()-t2}\")\n",
    "\n",
    "\n",
    "ep_dict[\"observation.state.qpos\"].append(observation[\"state\"][\"qpos\"])\n",
    "ep_dict[\"observation.state.qvel\"].append(observation[\"state\"][\"qvel\"])\n",
    "ep_dict[\"observation.state.gripper\"].append(observation[\"state\"][\"gripper\"])\n",
    "ep_dict[\"timestamp\"].append(env.unwrapped.data.time)\n",
    "\n",
    "# Append dummy nans to the actions so we have the same number of samples can plot it\n",
    "ep_dict[\"action.qpos\"].append(np.array([np.nan] * len(action[\"qpos\"])))\n",
    "ep_dict[\"action.gripper\"].append(np.array([np.nan]))\n",
    "# ep_dict[\"reward\"].append(np.array([np.nan]))\n",
    "\n",
    "avg_reward = sum(ep_dict[\"reward\"])/len(ep_dict[\"reward\"])\n",
    "print(f\"Avg reward: {avg_reward}\")\n",
    "media.show_video(frames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action[\"qpos\"] = calculate_action(nn, dataset, key='action.qpos').numpy()\n",
    "print(action[\"qpos\"])\n",
    "for n in nn:\n",
    "  print(dataset[n[1]][\"action.qpos\"])\n",
    "\n",
    "observation, info = env.reset(qpos=action[\"qpos\"])\n",
    "media.show_image(observation[\"pixels\"][\"side\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future actions\n",
    "Get the action some short time ahead to stop getting stuck in a local minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "k=3\n",
    "frames = []\n",
    "nn_frames = [[] for _ in range(k)]\n",
    "action = {}\n",
    "ep_dict = {\"action.qpos\": [], \"action.gripper\": [], \"observation.state.qpos\": [], \"observation.state.qvel\": [], \"observation.state.gripper\": [], \"observation.pixels.side\": [], \"observation.pixels.gripper\": [], \"reward\": [], \"timestamp\": [], \"frame_index\": [],}\n",
    "\n",
    "vector_data = dataset[\"observation.vector.side\"]\n",
    "\n",
    "step = 0\n",
    "done = False\n",
    "net.eval()\n",
    "while step < 50:\n",
    "\n",
    "  t0 = time.time()\n",
    "  image_side = (torch.from_numpy(observation[\"pixels\"][\"side\"]).permute(2, 0, 1).unsqueeze(0) / 255).to(device)\n",
    "  # image_gripper = torch.from_numpy(observation[\"pixels\"][\"gripper\"]).permute(2, 0, 1).unsqueeze(0) / 255\n",
    "\n",
    "  with torch.inference_mode():\n",
    "    embedding_side = net(image_side).squeeze().cpu()\n",
    "    # embedding_gripper = net(image_side, image_gripper).to(\"cpu\")\n",
    "  t1 = time.time()\n",
    "\n",
    "  # Find nearest neighbours to qpos\n",
    "  qpos_nn = calculate_nearest_neighbours(torch.from_numpy(observation[\"state\"][\"qpos\"]), dataset[\"observation.state.qpos\"], k=30)\n",
    "  qpos_candidates = [n[1] for n in qpos_nn]\n",
    "  \n",
    "  # Find most visually similar\n",
    "  vis_nn = calculate_nearest_neighbours(embedding_side, dataset[qpos_candidates][\"observation.vector.side\"], k=k)\n",
    "  \n",
    "  # HERE is where we look ahead\n",
    "  # Just going x rows ahead in the data is crude and does not deal with edge case of being at the end of a demo and accidentally skipping ahead to the next one\n",
    "  # But as a proof of concept it's good enough\n",
    "  nn = [(n[0],qpos_candidates[n[1]]+10) for n in vis_nn]\n",
    "  print(nn)\n",
    "  t2 = time.time()\n",
    "\n",
    "  # Save the nearest image in the dataset for comparison\n",
    "  stacked_frame = np.hstack([observation[\"pixels\"][\"side\"]] + [dataset[nn[i][1]][\"observation.pixels.side\"].permute(1, 2, 0).numpy() for i in range(k)])\n",
    "  frames.append(stacked_frame)\n",
    "\n",
    "  action[\"qpos\"] = calculate_action(nn, dataset, key='action.qpos').numpy()\n",
    "  action[\"gripper\"] = round(calculate_action(nn, dataset, key='action.gripper').item())\n",
    "  print(action)\n",
    "\n",
    "  \n",
    "  ep_dict[\"observation.state.qpos\"].append(observation[\"state\"][\"qpos\"])\n",
    "  ep_dict[\"observation.state.qvel\"].append(observation[\"state\"][\"qvel\"])\n",
    "  ep_dict[\"observation.state.gripper\"].append(observation[\"state\"][\"gripper\"])\n",
    "  ep_dict[\"timestamp\"].append(env.unwrapped.data.time)\n",
    "\n",
    "  # Step through the environment and receive a new observation\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "  \n",
    "  # ep_dict[\"observation.pixels.side\"].append(observation[\"pixels\"][\"side\"])\n",
    "  # ep_dict[\"observation.pixels.gripper\"].append(observation[\"pixels\"][\"gripper\"])\n",
    "  ep_dict[\"reward\"].append(reward)\n",
    "  ep_dict[\"action.qpos\"].append(action[\"qpos\"])\n",
    "  ep_dict[\"action.gripper\"].append(action[\"gripper\"])\n",
    "  # ep_dict[\"frame_index\"].append(step)\n",
    "  # action = calculate_action(nn, key='observation.state.qvel')\n",
    "    \n",
    "  done = truncated | done | terminated\n",
    "  step += 1\n",
    "\n",
    "  print(f\"Timing: inference: {t1-t0}, nn: {t2-t1}, rest: {time.time()-t2}\")\n",
    "\n",
    "\n",
    "ep_dict[\"observation.state.qpos\"].append(observation[\"state\"][\"qpos\"])\n",
    "ep_dict[\"observation.state.qvel\"].append(observation[\"state\"][\"qvel\"])\n",
    "ep_dict[\"observation.state.gripper\"].append(observation[\"state\"][\"gripper\"])\n",
    "ep_dict[\"timestamp\"].append(env.unwrapped.data.time)\n",
    "\n",
    "# Append dummy nans to the actions so we have the same number of samples can plot it\n",
    "ep_dict[\"action.qpos\"].append(np.array([np.nan] * len(action[\"qpos\"])))\n",
    "ep_dict[\"action.gripper\"].append(np.array([np.nan]))\n",
    "# ep_dict[\"reward\"].append(np.array([np.nan]))\n",
    "\n",
    "avg_reward = sum(ep_dict[\"reward\"])/len(ep_dict[\"reward\"])\n",
    "print(f\"Avg reward: {avg_reward}\")\n",
    "media.show_video(frames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lerobot_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
