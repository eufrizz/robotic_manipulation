{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VINN - Relative velocty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from lerobot.common.datasets.utils import hf_transform_to_torch\n",
    "# from lerobot.common.datasets.lerobot_dataset import LeRobotDataset, CODEBASE_VERSION\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_path = \"../datasets/grasp_100_2024-09-06_17-03-47.hf\"\n",
    "dataset = load_from_disk(dataset_path)\n",
    "# if \"from\" not in dataset.column_names:\n",
    "#   first_frames=dataset.filter(lambda example: example['frame_index'] == 0)\n",
    "#   from_idxs = torch.tensor(first_frames['index'])\n",
    "#   to_idxs = torch.tensor(first_frames['index'][1:] + [len(dataset)])\n",
    "#   episode_data_index={\"from\": from_idxs, \"to\": to_idxs}\n",
    "    \n",
    "dataset.set_transform(hf_transform_to_torch)\n",
    "# dataset = dataset.with_format(\"torch\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "        dataset,\n",
    "        num_workers=4,\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        # sampler=sampler,\n",
    "        # pin_memory=params[\"device\"].type != \"cpu\",\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "\n",
    "curr_time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "dataset_name = Path(dataset_path).stem\n",
    "CKPT_DIR=f'ckpts/resnet_byol_{dataset_name}_{curr_time}'\n",
    "TENSORBOARD_DIR=f'runs/resnet_byol_{dataset_name}_{curr_time}'\n",
    "Path(CKPT_DIR).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from byol_pytorch import BYOL\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\")\n",
    "\n",
    "# Remove the fc/classification layer\n",
    "resnet = torchvision.models.resnet18(weights='DEFAULT')\n",
    "modules = list(resnet.children())[:-1]\n",
    "backbone = torch.nn.Sequential(*modules)\n",
    "net = backbone\n",
    "\n",
    "\n",
    "learner = BYOL(\n",
    "    net,\n",
    "    image_size = 240,\n",
    "    hidden_layer = 8\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = torch.optim.Adam(learner.parameters(), lr=3e-4)\n",
    "\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_DIR)\n",
    "step = 0\n",
    "for epoch in range(1, 101):\n",
    "  print(f\"epoch {epoch}\")\n",
    "  end = time.time()\n",
    "  for batch in tqdm(dataloader):\n",
    "    data_load_time = time.time()\n",
    "    step +=1\n",
    "\n",
    "    # images = torch.cat((batch['observation.pixels.side'], batch['observation.pixels.gripper']), dim=0).to(device)\n",
    "    images = batch['observation.pixels.gripper'].to(device)\n",
    "    gpu_load_time = time.time()\n",
    "\n",
    "    loss = learner(images)\n",
    "    pred_time = time.time()\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    learner.update_moving_average() # update moving average of target encoder\n",
    "    train_time = time.time()\n",
    "\n",
    "    writer.add_scalar(\"Loss/train\", loss.item(), step)\n",
    "    writer.add_scalar(\"Time/data_load\", data_load_time - end, step)\n",
    "    writer.add_scalar(\"Time/gpu_transfer\", gpu_load_time - data_load_time, step)\n",
    "    writer.add_scalar(\"Time/pred_time\", pred_time - gpu_load_time, step)\n",
    "    writer.add_scalar(\"Time/train_time\", train_time - pred_time, step)\n",
    "    writer.add_scalar(\"Time/step_time\", time.time() - end, step)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      # save the improved network\n",
    "      torch.save({'policy_state_dict': net.state_dict(),\n",
    "                  'optimizer_state_dict': opt.state_dict(),\n",
    "                  'loss': loss,\n",
    "                  'epoch': epoch,\n",
    "                  'step': step,\n",
    "                  }, CKPT_DIR + f'/epoch_{epoch}.pt')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules = list(net.children())[:-1]\n",
    "# backbone = torch.nn.Sequential(*modules).to(device)\n",
    "\n",
    "# Save embeddings of all images\n",
    "img_cols = {\"observation.pixels.side\": \"observation.vector.side\", \"observation.pixels.gripper\" : \"observation.vector.gripper\"}\n",
    "vec_columns = {v:[] for k,v in img_cols.items()}\n",
    "for batch in tqdm(dataloader):\n",
    "    with torch.inference_mode():\n",
    "      for img_col in img_cols:\n",
    "        # Batch inference to get embeddings\n",
    "        embedding = net(batch[img_col].to(device)).to(\"cpu\")\n",
    "        # Add the embedding (size 512) for each image to the column\n",
    "        vec_columns[img_cols[img_col]].extend([e.numpy() for e in embedding.squeeze()])\n",
    "\n",
    "# vec_columns = {k: [r.numpy() for r in v] for k, v in vec_columns.items()}\n",
    "for col in vec_columns:\n",
    "  assert(len(vec_columns[col]) == len(dataset))\n",
    "  dataset = dataset.add_column(name=col, column=vec_columns[col])\n",
    "\n",
    "dataset.set_transform(None)\n",
    "dataset.save_to_disk(f\"../datasets/byol/{dataset_name}_epoch_{epoch}.hf\")\n",
    "dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed images in a dataset\n",
    "Run from here if you already have a pretrained model, but a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from lerobot.common.datasets.utils import hf_transform_to_torch\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_path = \"../datasets/grasp/grasp_ee_vel_random_50_2024-10-15_18-56-26.hf\"\n",
    "dataset_name = Path(dataset_path).stem\n",
    "dataset = load_from_disk(dataset_path)\n",
    "dataset.set_transform(hf_transform_to_torch)\n",
    "\n",
    "checkpoint = torch.load(\"../VINN/ckpts/resnet_byol_grasp_100_2024-09-06_17-03-47_2024-10-15_21-08/epoch_100.pt\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\")\n",
    "\n",
    "# Remove the fc/classification layer\n",
    "resnet = torchvision.models.resnet18().to(device)\n",
    "modules = list(resnet.children())[:-1]\n",
    "backbone = torch.nn.Sequential(*modules)\n",
    "net = backbone\n",
    "net.load_state_dict(checkpoint[\"policy_state_dict\"])\n",
    "net = net.to(device)\n",
    "\n",
    "net.eval()\n",
    "# Save embeddings of all images\n",
    "img_cols = {\"observation.pixels.side\": \"observation.vector.side\", \"observation.pixels.gripper\" : \"observation.vector.gripper\"}\n",
    "# img_cols = {\"observation.pixels.gripper\" : \"observation.vector.gripper\"}\n",
    "vec_columns = {v:[] for k,v in img_cols.items()}\n",
    "# For some reason, batch embedding fucks up and gives different results to single\n",
    "dataloader = DataLoader(\n",
    "        dataset,\n",
    "        num_workers=4,\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        # sampler=sampler,\n",
    "        # pin_memory=params[\"device\"].type != \"cpu\",\n",
    "        drop_last=False,\n",
    "    )\n",
    "for batch in tqdm(dataloader):\n",
    "    with torch.inference_mode():\n",
    "      for img_col in img_cols:\n",
    "        # Batch inference to get embeddings\n",
    "        embedding = net(batch[img_col].to(device)).to(\"cpu\")\n",
    "        # Add the embedding (size 512) for each image to the column\n",
    "        vec_columns[img_cols[img_col]].extend([e.numpy() for e in embedding.squeeze()])\n",
    "\n",
    "# for datum in tqdm(dataset):\n",
    "#     with torch.inference_mode():\n",
    "#       for img_col in img_cols:\n",
    "#         # Batch inference to get embeddings\n",
    "#         embedding = net(datum[img_col].unsqueeze(0).to(device)).cpu().squeeze()\n",
    "#         # Add the embedding (size 512) for each image to the column\n",
    "#         vec_columns[img_cols[img_col]].append(embedding.numpy())\n",
    "\n",
    "# vec_columns = {k: [r.numpy() for r in v] for k, v in vec_columns.items()}\n",
    "for col in vec_columns:\n",
    "  assert(len(vec_columns[col]) == len(dataset))\n",
    "  dataset = dataset.add_column(name=col, column=vec_columns[col])\n",
    "\n",
    "dataset.set_transform(None)\n",
    "dataset.save_to_disk(f\"../datasets/byol/{dataset_name}_epoch_{epoch}.hf\")\n",
    "dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in vec_columns:\n",
    "  assert(len(vec_columns[col]) == len(dataset))\n",
    "  dataset = dataset.add_column(name=col, column=vec_columns[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_transform(None)\n",
    "dataset.save_to_disk(f\"../datasets/byol/{dataset_name}_epoch_{epoch}.hf\")\n",
    "dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run VINN\n",
    "Start from here if you already have a vision model and a dataset with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and model if not continuing from before\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from lerobot.common.datasets.utils import hf_transform_to_torch\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "dataset_path = \"../datasets/byol/grasp_ee_vel_random_50_2024-10-15_18-56-26_epoch_100.hf\"\n",
    "dataset_name = Path(dataset_path).stem\n",
    "dataset = load_from_disk(dataset_path)\n",
    "# dataset.set_transform(hf_transform_to_torch)\n",
    "# dataset = dataset.with_format(\"torch\", device=device)\n",
    "dataset.set_format(\"torch\")\n",
    "\n",
    "checkpoint = torch.load(\"../VINN/ckpts/resnet_byol_grasp_100_2024-09-06_17-03-47_2024-10-15_21-08/epoch_100.pt\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\")\n",
    "\n",
    "# Remove the fc/classification layer\n",
    "resnet = torchvision.models.resnet18().to(device)\n",
    "modules = list(resnet.children())[:-1]\n",
    "backbone = torch.nn.Sequential(*modules)\n",
    "net = backbone\n",
    "net.load_state_dict(checkpoint[\"policy_state_dict\"])\n",
    "net = net.to(device)\n",
    "net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_lite6.env, gym_lite6.pickup_task, gym_lite6.utils\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import mediapy as media\n",
    "\n",
    "\n",
    "task = gym_lite6.pickup_task.GraspAndLiftTask('gripper_left_finger', 'gripper_right_finger', 'box', 'floor')\n",
    "env = gym.make(\n",
    "    \"UfactoryCubePickup-v0\",\n",
    "    task=task,\n",
    "    obs_type=\"pixels_state\",\n",
    "    action_type=\"qvel\",\n",
    "    max_episode_steps=300,\n",
    "    visualization_width=320,\n",
    "    visualization_height=240\n",
    ")\n",
    "observation, info = env.reset()\n",
    "media.show_image(env.render())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def dist_metric(x,y):\n",
    "  \"\"\"\n",
    "  L2 norm\n",
    "  \"\"\"\n",
    "  return torch.norm(x-y, dim=1)\n",
    "\n",
    "def calculate_nearest_neighbours(query, data, k=1, verbose=False):\n",
    "  t0 = time.time()\n",
    "  dists = []\n",
    "  assert len(data[0].shape) == 1, f\"Invalid shape, should be 1D: {data.shape}\"\n",
    "  # Takes around 0.25s for 20000 samples\n",
    "  dists = dist_metric(query, data).cpu()\n",
    "  heap = [(dists[i].item(), i) for i in range(data.shape[0])]\n",
    "  \n",
    "  t1 = time.time()\n",
    "  heapq.heapify(heap)\n",
    "  t2 = time.time()\n",
    "\n",
    "  out = []\n",
    "  for i in range(k):\n",
    "    out.append(heapq.heappop(heap))\n",
    "  \n",
    "  if verbose:\n",
    "    print(f\"Times: get dists: {t1-t0}, heapify: {t2-t1}\")\n",
    "  return out\n",
    "        \n",
    "def calculate_action(dists, dataset, key='action.qpos'):\n",
    "  \"\"\"\n",
    "  Local weighting\n",
    "  \"\"\"\n",
    "  if len(dists) > 1:\n",
    "    softmin = torch.nn.Softmin(dim=0)\n",
    "    top_k_weights = softmin(torch.tensor([d[0] for d in dists])).tolist()\n",
    "    action = sum([top_k_weights[i] * dataset[dists[i][1]][key] for i in range(len(dists))])\n",
    "  else:\n",
    "    action = dataset[dists[0][1]][key]\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "k=5\n",
    "frames = []\n",
    "nn_frames = [[] for _ in range(k)]\n",
    "action = {}\n",
    "ep_dict = {\n",
    "      \"action.qpos\": [], \"action.qvel\": [], \"action.gripper\": [],\"action.ee_vel\": [], \"action.ee_ang_vel\": [],\n",
    "      \"observation.state.qpos\": [], \"observation.state.qvel\": [], \"observation.state.gripper\": [], \"observation.pixels.side\": [], \"observation.pixels.gripper\": [],\n",
    "      \"observation.ee_pose.pos\": [], \"observation.ee_pose.quat\": [], \"observation.ee_pose.vel\": [], \"observation.ee_pose.ang_vel\": [],\n",
    "      \"reward\": [], \"timestamp\": [], \"frame_index\": [],\n",
    "      }\n",
    "# vector_data = dataset[\"observation.vector.side\"].to(device)\n",
    "vector_data = dataset[\"observation.vector.gripper\"].to(device)\n",
    "\n",
    "step = 0\n",
    "done = False\n",
    "net.eval()\n",
    "while step < 100:\n",
    "\n",
    "  t0 = time.time()\n",
    "  # image_side = (torch.from_numpy(observation[\"pixels\"][\"side\"]).permute(2, 0, 1).unsqueeze(0) / 255).to(device)\n",
    "  image_gripper = (torch.from_numpy(observation[\"pixels\"][\"gripper\"]).permute(2, 0, 1).unsqueeze(0) / 255).to(device)\n",
    "\n",
    "  with torch.inference_mode():\n",
    "    embedding = net(image_gripper).squeeze()\n",
    "    # embedding = net(image_gripper).squeeze()\n",
    "  t1 = time.time()\n",
    "  \n",
    "  nn = calculate_nearest_neighbours(embedding, vector_data, k=k)\n",
    "\n",
    "  t2 = time.time()\n",
    "\n",
    "  # Save the nearest image in the dataset for comparison\n",
    "  # stacked_frame = np.hstack([observation[\"pixels\"][\"side\"]] + [dataset[nn[i][1]][\"observation.pixels.side\"].permute(1, 2, 0).cpu().numpy() for i in range(k)])\n",
    "  stacked_frame = np.hstack([observation[\"pixels\"][\"gripper\"]] + [dataset[nn[i][1]][\"observation.pixels.gripper\"].permute(1, 2, 0).cpu().numpy() for i in range(k)])\n",
    "  frames.append(stacked_frame)\n",
    "\n",
    "  vel = calculate_action(nn, dataset, key='action.ee_vel').numpy()\n",
    "  ang_vel = calculate_action(nn, dataset, key='action.ee_ang_vel').numpy()\n",
    "  action[\"gripper\"] = round(calculate_action(nn, dataset, key='action.gripper').item())\n",
    "  action[\"qvel\"] = env.unwrapped.solve_ik_vel(vel, ang_vel, ref_frame='end_effector', local=True)\n",
    "\n",
    "  \n",
    "  ep_dict[\"observation.state.qpos\"].append(observation[\"state\"][\"qpos\"])\n",
    "  ep_dict[\"observation.state.qvel\"].append(observation[\"state\"][\"qvel\"])\n",
    "  ep_dict[\"observation.state.gripper\"].append(observation[\"state\"][\"gripper\"])\n",
    "  ep_dict[\"observation.pixels.side\"].append(observation[\"pixels\"][\"side\"])\n",
    "  ep_dict[\"observation.pixels.gripper\"].append(observation[\"pixels\"][\"gripper\"])\n",
    "  ep_dict[\"observation.ee_pose.pos\"].append(observation[\"ee_pose\"][\"pos\"])\n",
    "  ep_dict[\"observation.ee_pose.quat\"].append(observation[\"ee_pose\"][\"quat\"])\n",
    "  ep_dict[\"observation.ee_pose.vel\"].append(observation[\"ee_pose\"][\"vel\"])\n",
    "  ep_dict[\"observation.ee_pose.ang_vel\"].append(observation[\"ee_pose\"][\"ang_vel\"])\n",
    "  ep_dict[\"timestamp\"].append(env.unwrapped.data.time)\n",
    "\n",
    "  # Step through the environment and receive a new observation\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "  \n",
    "  ep_dict[\"reward\"].append(reward)\n",
    "  # ep_dict[\"action.qpos\"].append(action[\"qpos\"])\n",
    "  ep_dict[\"action.qvel\"].append(action[\"qvel\"])\n",
    "  ep_dict[\"action.ee_vel\"].append(vel)\n",
    "  ep_dict[\"action.ee_ang_vel\"].append(ang_vel)\n",
    "  ep_dict[\"action.gripper\"].append(action[\"gripper\"])\n",
    "    \n",
    "  done = truncated | done | terminated\n",
    "  step += 1\n",
    "\n",
    "  print(f\"Timing: inference: {t1-t0}, nn: {t2-t1}, rest: {time.time()-t2}\")\n",
    "\n",
    "\n",
    "ep_dict[\"observation.state.qpos\"].append(observation[\"state\"][\"qpos\"])\n",
    "ep_dict[\"observation.state.qvel\"].append(observation[\"state\"][\"qvel\"])\n",
    "ep_dict[\"observation.state.gripper\"].append(observation[\"state\"][\"gripper\"])\n",
    "ep_dict[\"observation.pixels.side\"].append(observation[\"pixels\"][\"side\"])\n",
    "ep_dict[\"observation.pixels.gripper\"].append(observation[\"pixels\"][\"gripper\"])\n",
    "ep_dict[\"observation.ee_pose.pos\"].append(observation[\"ee_pose\"][\"pos\"])\n",
    "ep_dict[\"observation.ee_pose.quat\"].append(observation[\"ee_pose\"][\"quat\"])\n",
    "ep_dict[\"observation.ee_pose.vel\"].append(observation[\"ee_pose\"][\"vel\"])\n",
    "ep_dict[\"observation.ee_pose.ang_vel\"].append(observation[\"ee_pose\"][\"ang_vel\"])\n",
    "ep_dict[\"timestamp\"].append(env.unwrapped.data.time)\n",
    "\n",
    "# Append dummy nans to the actions so we have the same number of samples can plot it\n",
    "# ep_dict[\"action.qpos\"].append(np.array([np.nan] * len(action[\"qpos\"])))\n",
    "ep_dict[\"action.gripper\"].append(np.array([np.nan]))\n",
    "ep_dict[\"action.qvel\"].append(np.array([np.nan] * len(action[\"qvel\"])))\n",
    "ep_dict[\"action.ee_vel\"].append(np.array([np.nan] * len(vel)))\n",
    "ep_dict[\"action.ee_ang_vel\"].append(np.array([np.nan] * len(ang_vel)))\n",
    "# ep_dict[\"reward\"].append(np.array([np.nan]))\n",
    "\n",
    "avg_reward = sum(ep_dict[\"reward\"])/len(ep_dict[\"reward\"])\n",
    "print(f\"Avg reward: {avg_reward}\")\n",
    "media.show_video(frames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_lite6.utils.plot_dict_of_arrays(ep_dict, \"timestamp\", keys=[\"action.qpos\", \"observation.state.qpos\"], sharey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(dataset['observation.vector.gripper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x=X_pca[:, 0], y=X_pca[:, 1], marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(dataset['observation.vector.gripper'])\n",
    "tsne.kl_divergence_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=X_tsne[:, 0], y=X_tsne[:, 1], marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['observation.vector.gripper'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_data = torch.from_numpy(X_tsne).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = index.search(dataset[0]['observation.vector.gripper'].numpy(), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from usearch.index import Index\n",
    "observation, info = env.reset()\n",
    "k=5\n",
    "frames = []\n",
    "side_frames = []\n",
    "nn_frames = [[] for _ in range(k)]\n",
    "action = {}\n",
    "ep_dict = {\n",
    "      \"action.qpos\": [], \"action.qvel\": [], \"action.gripper\": [],\"action.ee_vel\": [], \"action.ee_ang_vel\": [],\n",
    "      \"observation.state.qpos\": [], \"observation.state.qvel\": [], \"observation.state.gripper\": [], \"observation.pixels.side\": [], \"observation.pixels.gripper\": [],\n",
    "      \"observation.ee_pose.pos\": [], \"observation.ee_pose.quat\": [], \"observation.ee_pose.vel\": [], \"observation.ee_pose.ang_vel\": [],\n",
    "      \"reward\": [], \"timestamp\": [], \"frame_index\": [],\n",
    "      }\n",
    "vector_data = dataset[\"observation.vector.gripper\"].to(device)\n",
    "index = Index(ndim=dataset['observation.vector.gripper'].shape[1])\n",
    "index.add(vectors=dataset['observation.vector.gripper'].numpy(), keys=None)\n",
    "# vector_data = torch.from_numpy(X_tsne).to(device)\n",
    "\n",
    "step = 0\n",
    "done = False\n",
    "net.eval()\n",
    "while not done:\n",
    "\n",
    "  t0 = time.time()\n",
    "  # image_side = (torch.from_numpy(observation[\"pixels\"][\"side\"]).permute(2, 0, 1).unsqueeze(0) / 255).to(device)\n",
    "  image_gripper = (torch.from_numpy(observation[\"pixels\"][\"gripper\"]).permute(2, 0, 1).unsqueeze(0) / 255).to(device)\n",
    "\n",
    "  with torch.inference_mode():\n",
    "    embedding = net(image_gripper).squeeze()\n",
    "    # embedding = net(image_gripper).squeeze()\n",
    "  t1 = time.time()\n",
    "  \n",
    "  matches = index.search(embedding.cpu().numpy(), count=k)\n",
    "  nn = [(matches.distances[i], int(matches.keys[i])) for i in range(k)]\n",
    "\n",
    "  t2 = time.time()\n",
    "\n",
    "  # Save the nearest image in the dataset for comparison\n",
    "  # stacked_frame = np.hstack([observation[\"pixels\"][\"side\"]] + [dataset[nn[i][1]][\"observation.pixels.side\"].permute(1, 2, 0).cpu().numpy() for i in range(k)])\n",
    "  stacked_frame = np.hstack([observation[\"pixels\"][\"gripper\"]] + [dataset[nn[i][1]][\"observation.pixels.gripper\"].permute(1, 2, 0).cpu().numpy() for i in range(k)])\n",
    "  frames.append(stacked_frame)\n",
    "  side_frames.append(observation[\"pixels\"][\"side\"])\n",
    "\n",
    "  vel = calculate_action(nn, dataset, key='action.ee_vel').numpy()\n",
    "  ang_vel = calculate_action(nn, dataset, key='action.ee_ang_vel').numpy()\n",
    "  action[\"gripper\"] = round(calculate_action(nn, dataset, key='action.gripper').item())\n",
    "  action[\"qvel\"] = env.unwrapped.solve_ik_vel(vel, ang_vel, ref_frame='end_effector', local=True)\n",
    "\n",
    "  \n",
    "  ep_dict[\"observation.state.qpos\"].append(observation[\"state\"][\"qpos\"])\n",
    "  ep_dict[\"observation.state.qvel\"].append(observation[\"state\"][\"qvel\"])\n",
    "  ep_dict[\"observation.state.gripper\"].append(observation[\"state\"][\"gripper\"])\n",
    "  ep_dict[\"observation.pixels.side\"].append(observation[\"pixels\"][\"side\"])\n",
    "  ep_dict[\"observation.pixels.gripper\"].append(observation[\"pixels\"][\"gripper\"])\n",
    "  ep_dict[\"observation.ee_pose.pos\"].append(observation[\"ee_pose\"][\"pos\"])\n",
    "  ep_dict[\"observation.ee_pose.quat\"].append(observation[\"ee_pose\"][\"quat\"])\n",
    "  ep_dict[\"observation.ee_pose.vel\"].append(observation[\"ee_pose\"][\"vel\"])\n",
    "  ep_dict[\"observation.ee_pose.ang_vel\"].append(observation[\"ee_pose\"][\"ang_vel\"])\n",
    "  ep_dict[\"timestamp\"].append(env.unwrapped.data.time)\n",
    "\n",
    "  # Step through the environment and receive a new observation\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "  \n",
    "  ep_dict[\"reward\"].append(reward)\n",
    "  # ep_dict[\"action.qpos\"].append(action[\"qpos\"])\n",
    "  ep_dict[\"action.qvel\"].append(action[\"qvel\"])\n",
    "  ep_dict[\"action.ee_vel\"].append(vel)\n",
    "  ep_dict[\"action.ee_ang_vel\"].append(ang_vel)\n",
    "  ep_dict[\"action.gripper\"].append(action[\"gripper\"])\n",
    "    \n",
    "  done = truncated | done | terminated\n",
    "  step += 1\n",
    "\n",
    "  print(f\"Timing: inference: {t1-t0}, nn: {t2-t1}, rest: {time.time()-t2}\")\n",
    "\n",
    "\n",
    "ep_dict[\"observation.state.qpos\"].append(observation[\"state\"][\"qpos\"])\n",
    "ep_dict[\"observation.state.qvel\"].append(observation[\"state\"][\"qvel\"])\n",
    "ep_dict[\"observation.state.gripper\"].append(observation[\"state\"][\"gripper\"])\n",
    "ep_dict[\"observation.pixels.side\"].append(observation[\"pixels\"][\"side\"])\n",
    "ep_dict[\"observation.pixels.gripper\"].append(observation[\"pixels\"][\"gripper\"])\n",
    "ep_dict[\"observation.ee_pose.pos\"].append(observation[\"ee_pose\"][\"pos\"])\n",
    "ep_dict[\"observation.ee_pose.quat\"].append(observation[\"ee_pose\"][\"quat\"])\n",
    "ep_dict[\"observation.ee_pose.vel\"].append(observation[\"ee_pose\"][\"vel\"])\n",
    "ep_dict[\"observation.ee_pose.ang_vel\"].append(observation[\"ee_pose\"][\"ang_vel\"])\n",
    "ep_dict[\"timestamp\"].append(env.unwrapped.data.time)\n",
    "\n",
    "# Append dummy nans to the actions so we have the same number of samples can plot it\n",
    "# ep_dict[\"action.qpos\"].append(np.array([np.nan] * len(action[\"qpos\"])))\n",
    "ep_dict[\"action.gripper\"].append(np.array([np.nan]))\n",
    "ep_dict[\"action.qvel\"].append(np.array([np.nan] * len(action[\"qvel\"])))\n",
    "ep_dict[\"action.ee_vel\"].append(np.array([np.nan] * len(vel)))\n",
    "ep_dict[\"action.ee_ang_vel\"].append(np.array([np.nan] * len(ang_vel)))\n",
    "# ep_dict[\"reward\"].append(np.array([np.nan]))\n",
    "\n",
    "avg_reward = sum(ep_dict[\"reward\"])/len(ep_dict[\"reward\"])\n",
    "print(f\"Avg reward: {avg_reward}\")\n",
    "media.show_video(frames)\n",
    "media.show_video(side_frames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_lite6.utils.plot_dict_of_arrays(ep_dict, \"timestamp\", keys=[\"action.ee_vel\", \"action.ee_ang_vel\", \"observation.state.qpos\"], sharey=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important and useful test to make sure embeddings can be reproduced\n",
    "idx = 20\n",
    "img = (dataset[idx][\"observation.pixels.gripper\"]/255).unsqueeze(0).to(device)\n",
    "dataset_embedding = dataset[idx][\"observation.vector.gripper\"].numpy()\n",
    "\n",
    "net.eval()\n",
    "with torch.inference_mode():\n",
    "  embedding = net(img).squeeze()\n",
    "t1 = time.time()\n",
    "  \n",
    "matches_dataset = index.search(dataset_embedding, count=2)\n",
    "matches = index.search(embedding.cpu().numpy(), count=2)\n",
    "print(matches_dataset.keys, matches_dataset.distances)\n",
    "print(matches.keys, matches.distances)\n",
    "\n",
    "media.show_images([dataset[idx][\"observation.pixels.gripper\"].permute(1, 2, 0)])\n",
    "media.show_images([dataset[int(i)][\"observation.pixels.gripper\"].permute(1, 2, 0) for i in matches_dataset.keys])\n",
    "media.show_images([dataset[int(i)][\"observation.pixels.gripper\"].permute(1, 2, 0) for i in matches.keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[idx][\"observation.pixels.gripper\"]/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lerobot_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
